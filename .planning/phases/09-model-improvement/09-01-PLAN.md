---
phase: 09-model-improvement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/classifier.py
  - src/models/factory.py
  - scripts/benchmark.py
  - Makefile
autonomous: true

must_haves:
  truths:
    - "BTXRDClassifier accepts **kwargs and passes block_args through to timm.create_model for CBAM support"
    - "gradcam_target_layer returns model.layer4[-1] for ResNet architectures and model.bn2 for EfficientNet"
    - "create_model in factory.py reads model.attn_layer from config and passes block_args=dict(attn_layer=...) to BTXRDClassifier"
    - "benchmark.py orchestrates the experiment grid: 4 experiments x 2 splits, calling train.py + eval.py for each"
    - "Existing B0 checkpoints are backed up before any new training runs"
  artifacts:
    - path: "src/models/classifier.py"
      provides: "Multi-architecture BTXRDClassifier with ResNet + EfficientNet grad-cam support"
      contains: "layer4"
    - path: "src/models/factory.py"
      provides: "Model factory with attn_layer/block_args passthrough"
      contains: "block_args"
    - path: "scripts/benchmark.py"
      provides: "Experiment grid orchestration script"
      contains: "EXPERIMENT_GRID"
  key_links:
    - from: "src/models/factory.py"
      to: "src/models/classifier.py"
      via: "create_model passes **kwargs including block_args"
      pattern: "block_args"
    - from: "scripts/benchmark.py"
      to: "scripts/train.py"
      via: "subprocess calls with --override arguments"
      pattern: "train\\.py"
---

<objective>
Add multi-architecture support (ResNet-50-CBAM, EfficientNet-B3) to the classifier and factory, create a benchmark orchestration script to run the full experiment grid, and back up existing B0 checkpoints.

Purpose: Enable systematic comparison of architectures and annotation-guided training to close the accuracy gap with the BTXRD paper baseline (67.9% -> 75%+ stratified target).

Output: Updated classifier.py and factory.py supporting ResNet+CBAM, a benchmark.py script ready to execute 4 experiments x 2 splits, and backed-up B0 checkpoints.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-model-improvement/09-RESEARCH.md
@src/models/classifier.py
@src/models/factory.py
@configs/default.yaml
@scripts/train.py
@scripts/eval.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update classifier.py and factory.py for multi-architecture support</name>
  <files>src/models/classifier.py, src/models/factory.py</files>
  <action>
**classifier.py changes:**
1. Update `BTXRDClassifier.__init__` signature to accept `**kwargs` after the existing named parameters
2. Pass `**kwargs` through to `timm.create_model()` so that `block_args=dict(attn_layer='cbam')` flows through
3. Update the `gradcam_target_layer` property to handle multiple architectures:
   - If `hasattr(self.model, 'bn2')`: return `self.model.bn2` (EfficientNet family)
   - Elif `hasattr(self.model, 'layer4')`: return `self.model.layer4[-1]` (ResNet family)
   - Else: fallback walk backwards (existing logic)
4. Update the class docstring to mention ResNet support, not just EfficientNet
5. Update the module-level docstring to reflect multi-architecture support

**factory.py changes:**
1. In `create_model()`, read `model_cfg.get("attn_layer")` from config
2. If `attn_layer` is set, build `kwargs = {"block_args": dict(attn_layer=model_cfg["attn_layer"])}`
3. Pass these `**kwargs` through to `BTXRDClassifier()`
4. Update docstring to document the new `attn_layer` config key

**Important:** Do NOT change the `save_checkpoint` or `load_checkpoint` functions. Do NOT change `compute_class_weights` or `EarlyStopping`. Only touch `create_model`, `BTXRDClassifier.__init__`, and `BTXRDClassifier.gradcam_target_layer`.

**Verification test (run in Python):**
```python
import timm
# Verify ResNet-50-CBAM creates successfully
model = timm.create_model('resnet50', pretrained=False, num_classes=3, drop_rate=0.3, block_args=dict(attn_layer='cbam'))
assert hasattr(model, 'layer4')
print(f"ResNet-50-CBAM params: {sum(p.numel() for p in model.parameters()):,}")
```
  </action>
  <verify>
Run: `python -c "from src.models.factory import create_model; m = create_model({'model': {'backbone': 'resnet50', 'num_classes': 3, 'pretrained': False, 'dropout': 0.3, 'attn_layer': 'cbam'}}); print(type(m.gradcam_target_layer).__name__); print('OK')"` -- should print "Bottleneck" then "OK".

Run: `python -c "from src.models.factory import create_model; m = create_model({'model': {'backbone': 'efficientnet_b3', 'num_classes': 3, 'pretrained': False, 'dropout': 0.3}}); print(type(m.gradcam_target_layer).__name__); print('OK')"` -- should print "BatchNormAct2d" then "OK".
  </verify>
  <done>BTXRDClassifier supports both EfficientNet and ResNet+CBAM architectures with correct grad-cam target layers. factory.py passes attn_layer config through as block_args.</done>
</task>

<task type="auto">
  <name>Task 2: Create benchmark.py orchestration script and update Makefile</name>
  <files>scripts/benchmark.py, Makefile</files>
  <action>
Create `scripts/benchmark.py` that orchestrates the experiment grid from the research phase.

**Experiment grid (hardcoded in script):**
```python
EXPERIMENT_GRID = [
    {
        "id": "E1_b0_baseline",
        "backbone": "efficientnet_b0",
        "image_size": 224,
        "batch_size": 32,
        "dropout": 0.2,
        "annotations": False,  # data.annotations_dir should be cleared
        "attn_layer": None,
    },
    {
        "id": "E2_b0_annotated",
        "backbone": "efficientnet_b0",
        "image_size": 224,
        "batch_size": 32,
        "dropout": 0.2,
        "annotations": True,
        "attn_layer": None,
    },
    {
        "id": "E3_b3_annotated",
        "backbone": "efficientnet_b3",
        "image_size": 300,
        "batch_size": 16,
        "dropout": 0.3,
        "annotations": True,
        "attn_layer": None,
    },
    {
        "id": "E4_resnet50_cbam",
        "backbone": "resnet50",
        "image_size": 224,
        "batch_size": 16,
        "dropout": 0.3,
        "annotations": True,
        "attn_layer": "cbam",
    },
]
```

**Script behavior:**
1. Accept `--config` (default: configs/default.yaml), `--experiments` (comma-separated IDs or "all", default: "all"), and `--skip-eval` flag
2. Before running any experiments, back up existing checkpoints: copy `checkpoints/*.pt` to `checkpoints/backup_b0_baseline/` if backup dir doesn't already exist
3. For each experiment in the grid:
   a. For each split strategy (stratified, center):
      - Build `--override` arguments from the experiment config:
        - `model.backbone={backbone}`
        - `data.image_size={image_size}`
        - `training.batch_size={batch_size}`
        - `model.dropout={dropout}`
        - `training.split_strategy={split}`
        - If `annotations` is False: `data.annotations_dir=null` (disables annotation-guided dataset)
        - If `attn_layer` is set: `model.attn_layer={attn_layer}`
      - Run `python scripts/train.py --config {config} --override ...` as a subprocess
      - After training, rename the checkpoint from `best_{split_prefix}.pt` to `best_{split_prefix}_{experiment_id}.pt`
      - If `--skip-eval` is NOT set, run `python scripts/eval.py` with appropriate config overrides (need to also override `inference.default_checkpoint` to point to the experiment's checkpoint)
      - Move results from `results/{split_dir}/` to `results/experiments/{experiment_id}/{split_dir}/`
   b. Log experiment status (success/failure) with timing
4. After all experiments complete, print a summary table of experiment statuses
5. Use `subprocess.run` with `check=True` (or catch CalledProcessError) to run each command
6. Use `logging` module for all output

**Important implementation details:**
- The eval.py script reads checkpoint paths from SPLIT_MAP (hardcoded). To evaluate experiment-specific checkpoints, the benchmark script should: (a) rename/copy the experiment checkpoint to the standard name before running eval, then rename back, OR (b) add a `--checkpoint` override to eval.py. The simpler approach is (a): temporarily symlink or copy the experiment checkpoint to the standard name, run eval, then move results.
- Actually the cleanest approach: after training each split, the checkpoint is saved as `best_{split_prefix}.pt`. Run eval immediately (before the next experiment overwrites it), then move both the checkpoint and results to the experiment directory.
- The default.yaml currently has `annotations_dir: data_raw/Annotations`. To disable annotations for E1, pass `--override data.annotations_dir=null`.

**Makefile update:**
Add a `benchmark` target:
```makefile
benchmark: ## Run model improvement benchmark (all experiments)
	python scripts/benchmark.py --config configs/default.yaml
```

Also update the `all` target to NOT include benchmark (it's optional/expensive).
  </action>
  <verify>
Run: `python scripts/benchmark.py --help` -- should show help with --config, --experiments, --skip-eval options.

Run: `python -c "import scripts.benchmark"` -- should import without errors (basic syntax check).

Run: `make help` -- should list the new `benchmark` target.
  </verify>
  <done>benchmark.py exists with EXPERIMENT_GRID, orchestrates train+eval for 4 experiments x 2 splits, backs up existing checkpoints, and moves results to per-experiment directories. Makefile has `benchmark` target.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.models.classifier import BTXRDClassifier; import torch; m = BTXRDClassifier('resnet50', 3, False, 0.3, block_args=dict(attn_layer='cbam')); x = torch.randn(1, 3, 224, 224); y = m(x); assert y.shape == (1, 3); print('ResNet-CBAM forward pass OK')"` passes
2. `python -c "from src.models.classifier import BTXRDClassifier; m = BTXRDClassifier('efficientnet_b3', 3, False, 0.3); print(type(m.gradcam_target_layer).__name__)"` prints "BatchNormAct2d"
3. `python -c "from src.models.classifier import BTXRDClassifier; m = BTXRDClassifier('resnet50', 3, False, 0.3, block_args=dict(attn_layer='cbam')); print(type(m.gradcam_target_layer).__name__)"` prints "Bottleneck"
4. `python scripts/benchmark.py --help` shows usage
5. `make help` lists `benchmark` target
</verification>

<success_criteria>
- BTXRDClassifier supports ResNet-50-CBAM with correct forward pass and grad-cam target layer
- factory.py reads attn_layer from config and passes as block_args
- benchmark.py can orchestrate the full experiment grid (verified by --help and import)
- Existing B0 checkpoints will be backed up before experiments run
- Makefile has benchmark target
</success_criteria>

<output>
After completion, create `.planning/phases/09-model-improvement/09-01-SUMMARY.md`
</output>
