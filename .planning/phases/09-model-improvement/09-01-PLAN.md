---
phase: 09-model-improvement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/classifier.py
  - src/models/factory.py
  - scripts/benchmark.py
  - scripts/eval.py
  - Makefile
autonomous: true

must_haves:
  truths:
    - "BTXRDClassifier accepts **kwargs and passes block_args through to timm.create_model for CBAM support"
    - "gradcam_target_layer returns model.layer4[-1] for ResNet architectures and model.bn2 for EfficientNet"
    - "create_model in factory.py reads model.attn_layer from config and passes block_args=dict(attn_layer=...) to BTXRDClassifier"
    - "benchmark.py orchestrates the experiment grid: 4 experiments x 2 splits, calling train.py + eval.py for each"
    - "eval.py accepts --checkpoint-dir override to evaluate experiment-specific checkpoints"
    - "benchmark.py generates results/experiments/comparison.csv after all experiments complete"
    - "Existing B0 checkpoints are backed up before any new training runs"
  artifacts:
    - path: "src/models/classifier.py"
      provides: "Multi-architecture BTXRDClassifier with ResNet + EfficientNet grad-cam support"
      contains: "layer4"
    - path: "src/models/factory.py"
      provides: "Model factory with attn_layer/block_args passthrough"
      contains: "block_args"
    - path: "scripts/benchmark.py"
      provides: "Experiment grid orchestration script with comparison table generation"
      contains: "EXPERIMENT_GRID"
    - path: "scripts/eval.py"
      provides: "Evaluation script with --checkpoint-dir override support"
      contains: "checkpoint_dir"
    - path: "results/experiments/comparison.csv"
      provides: "Cross-experiment comparison table"
  key_links:
    - from: "src/models/factory.py"
      to: "src/models/classifier.py"
      via: "create_model passes **kwargs including block_args"
      pattern: "block_args"
    - from: "scripts/benchmark.py"
      to: "scripts/train.py"
      via: "subprocess calls with --override arguments"
      pattern: "train\\.py"
    - from: "scripts/benchmark.py"
      to: "scripts/eval.py"
      via: "subprocess calls with --checkpoint-dir argument"
      pattern: "eval\\.py.*--checkpoint-dir"
---

<objective>
Add multi-architecture support (ResNet-50-CBAM, EfficientNet-B3) to the classifier and factory, create a benchmark orchestration script to run the full experiment grid, and back up existing B0 checkpoints.

Purpose: Enable systematic comparison of architectures and annotation-guided training to close the accuracy gap with the BTXRD paper baseline (67.9% -> 75%+ stratified target).

Output: Updated classifier.py and factory.py supporting ResNet+CBAM, a benchmark.py script ready to execute 4 experiments x 2 splits, and backed-up B0 checkpoints.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-model-improvement/09-RESEARCH.md
@src/models/classifier.py
@src/models/factory.py
@configs/default.yaml
@scripts/train.py
@scripts/eval.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update classifier.py and factory.py for multi-architecture support</name>
  <files>src/models/classifier.py, src/models/factory.py</files>
  <action>
**classifier.py changes:**
1. Update `BTXRDClassifier.__init__` signature to accept `**kwargs` after the existing named parameters
2. Pass `**kwargs` through to `timm.create_model()` so that `block_args=dict(attn_layer='cbam')` flows through
3. Update the `gradcam_target_layer` property to handle multiple architectures:
   - If `hasattr(self.model, 'bn2')`: return `self.model.bn2` (EfficientNet family)
   - Elif `hasattr(self.model, 'layer4')`: return `self.model.layer4[-1]` (ResNet family)
   - Else: fallback walk backwards (existing logic)
4. Update the class docstring to mention ResNet support, not just EfficientNet
5. Update the module-level docstring to reflect multi-architecture support

**factory.py changes:**
1. In `create_model()`, read `model_cfg.get("attn_layer")` from config
2. If `attn_layer` is set, build `kwargs = {"block_args": dict(attn_layer=model_cfg["attn_layer"])}`
3. Pass these `**kwargs` through to `BTXRDClassifier()`
4. Update docstring to document the new `attn_layer` config key

**Important:** Do NOT change the `save_checkpoint` or `load_checkpoint` functions. Do NOT change `compute_class_weights` or `EarlyStopping`. Only touch `create_model`, `BTXRDClassifier.__init__`, and `BTXRDClassifier.gradcam_target_layer`.

**Verification test (run in Python):**
```python
import timm
# Verify ResNet-50-CBAM creates successfully
model = timm.create_model('resnet50', pretrained=False, num_classes=3, drop_rate=0.3, block_args=dict(attn_layer='cbam'))
assert hasattr(model, 'layer4')
print(f"ResNet-50-CBAM params: {sum(p.numel() for p in model.parameters()):,}")
```
  </action>
  <verify>
Run: `python -c "from src.models.factory import create_model; m = create_model({'model': {'backbone': 'resnet50', 'num_classes': 3, 'pretrained': False, 'dropout': 0.3, 'attn_layer': 'cbam'}}); print(type(m.gradcam_target_layer).__name__); print('OK')"` -- should print "Bottleneck" then "OK".

Run: `python -c "from src.models.factory import create_model; m = create_model({'model': {'backbone': 'efficientnet_b3', 'num_classes': 3, 'pretrained': False, 'dropout': 0.3}}); print(type(m.gradcam_target_layer).__name__); print('OK')"` -- should print "BatchNormAct2d" then "OK".
  </verify>
  <done>BTXRDClassifier supports both EfficientNet and ResNet+CBAM architectures with correct grad-cam target layers. factory.py passes attn_layer config through as block_args.</done>
</task>

<task type="auto">
  <name>Task 2: Create benchmark.py orchestration script and update Makefile</name>
  <files>scripts/benchmark.py, Makefile</files>
  <action>
**Step A: Add --checkpoint-dir to eval.py** (5-line change in scripts/eval.py)

eval.py currently hardcodes checkpoint filenames via SPLIT_MAP (e.g., "best_stratified.pt", "best_center.pt"). Add a `--checkpoint-dir` CLI argument that, when provided, overrides the directory where eval.py looks for checkpoints (the filenames from SPLIT_MAP stay the same). This lets benchmark.py copy experiment checkpoints to a temp directory with the standard names and point eval.py there.

Implementation:
1. Add argument to the parser: `parser.add_argument("--checkpoint-dir", type=str, default=None, help="Override checkpoint directory (default: from config paths.checkpoints_dir)")`
2. After config is loaded, if `args.checkpoint_dir` is set, override: `cfg["paths"]["checkpoints_dir"] = args.checkpoint_dir`
3. That's it -- the existing SPLIT_MAP logic builds paths from `cfg["paths"]["checkpoints_dir"]` so it will automatically use the override directory.

**Step B: Create scripts/benchmark.py**

Create `scripts/benchmark.py` that orchestrates the experiment grid from the research phase.

**Experiment grid (hardcoded in script):**
```python
EXPERIMENT_GRID = [
    {
        "id": "E1_b0_baseline",
        "backbone": "efficientnet_b0",
        "image_size": 224,
        "batch_size": 32,
        "dropout": 0.2,
        "annotations": False,
        "attn_layer": None,
    },
    {
        "id": "E2_b0_annotated",
        "backbone": "efficientnet_b0",
        "image_size": 224,
        "batch_size": 32,
        "dropout": 0.2,
        "annotations": True,
        "attn_layer": None,
    },
    {
        "id": "E3_b3_annotated",
        "backbone": "efficientnet_b3",
        "image_size": 300,
        "batch_size": 16,
        "dropout": 0.3,
        "annotations": True,
        "attn_layer": None,
    },
    {
        "id": "E4_resnet50_cbam",
        "backbone": "resnet50",
        "image_size": 224,
        "batch_size": 16,
        "dropout": 0.3,
        "annotations": True,
        "attn_layer": "cbam",
    },
]
```

**Script behavior:**
1. Accept `--config` (default: configs/default.yaml), `--experiments` (comma-separated IDs or "all", default: "all"), and `--skip-eval` flag
2. Before running any experiments, back up existing checkpoints: copy `checkpoints/*.pt` to `checkpoints/backup_b0_baseline/` if backup dir doesn't already exist
3. For each experiment in the grid:
   a. For each split strategy (stratified, center):
      - Build `--override` arguments from the experiment config:
        - `model.backbone={backbone}`
        - `data.image_size={image_size}`
        - `training.batch_size={batch_size}`
        - `model.dropout={dropout}`
        - `training.split_strategy={split}`
        - If `annotations` is False: `data.annotations_dir=null` (disables annotation-guided dataset)
        - If `attn_layer` is set: `model.attn_layer={attn_layer}`
      - Run `python scripts/train.py --config {config} --override ...` as a subprocess
      - After training completes, the checkpoint is saved as `best_{split_prefix}.pt` in the default checkpoints dir
      - If `--skip-eval` is NOT set: create a temp checkpoint directory `results/experiments/{experiment_id}/checkpoints/`, copy `best_{split_prefix}.pt` there, then run `python scripts/eval.py --config {config} --checkpoint-dir results/experiments/{experiment_id}/checkpoints/ --override paths.results_dir=results/experiments/{experiment_id}`
      - Move/rename the checkpoint from the default location to `checkpoints/best_{split_prefix}_{experiment_id}.pt`
   b. Log experiment status (success/failure) with timing
4. After all experiments complete, generate comparison table (see below)
5. Print a summary table of experiment statuses
6. Use `subprocess.run` with `check=True` (or catch CalledProcessError) to run each command
7. Use `logging` module for all output

**Comparison table generation (built into benchmark.py):**

After all experiments complete, benchmark.py must generate `results/experiments/comparison.csv` and `results/experiments/comparison.json`:
1. Read `metrics_summary.json` from each `results/experiments/{experiment_id}/stratified/` and `results/experiments/{experiment_id}/center_holdout/` directory
2. Build a comparison table with columns: Experiment ID, Backbone, Image Size, Annotations, Stratified Accuracy, Stratified Macro AUC, Stratified Malignant Sensitivity, Center-Holdout Accuracy, Center-Holdout Macro AUC, Center-Holdout Malignant Sensitivity
3. Save as CSV and JSON
4. Determine the best experiment:
   - Primary: highest stratified accuracy
   - Tiebreaker 1: highest malignant sensitivity (stratified)
   - Tiebreaker 2: highest center-holdout accuracy
5. Print the comparison table to stdout with the winner highlighted
6. This comparison generation runs even if some experiments failed (report available results, note failures)

**Makefile update:**
Add a `benchmark` target:
```makefile
benchmark: ## Run model improvement benchmark (all experiments)
	python scripts/benchmark.py --config configs/default.yaml
```

Also update the `all` target to NOT include benchmark (it's optional/expensive).
  </action>
  <verify>
Run: `python scripts/eval.py --help` -- should show `--checkpoint-dir` in the help output.

Run: `python scripts/benchmark.py --help` -- should show help with --config, --experiments, --skip-eval options.

Run: `python -c "import scripts.benchmark"` -- should import without errors (basic syntax check).

Run: `make help` -- should list the new `benchmark` target.
  </verify>
  <done>eval.py accepts --checkpoint-dir override. benchmark.py exists with EXPERIMENT_GRID, orchestrates train+eval for 4 experiments x 2 splits, generates comparison.csv, backs up existing checkpoints, and moves results to per-experiment directories. Makefile has `benchmark` target.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.models.classifier import BTXRDClassifier; import torch; m = BTXRDClassifier('resnet50', 3, False, 0.3, block_args=dict(attn_layer='cbam')); x = torch.randn(1, 3, 224, 224); y = m(x); assert y.shape == (1, 3); print('ResNet-CBAM forward pass OK')"` passes
2. `python -c "from src.models.classifier import BTXRDClassifier; m = BTXRDClassifier('efficientnet_b3', 3, False, 0.3); print(type(m.gradcam_target_layer).__name__)"` prints "BatchNormAct2d"
3. `python -c "from src.models.classifier import BTXRDClassifier; m = BTXRDClassifier('resnet50', 3, False, 0.3, block_args=dict(attn_layer='cbam')); print(type(m.gradcam_target_layer).__name__)"` prints "Bottleneck"
4. `python scripts/eval.py --help` shows `--checkpoint-dir` option
5. `python scripts/benchmark.py --help` shows usage
6. `make help` lists `benchmark` target
</verification>

<success_criteria>
- BTXRDClassifier supports ResNet-50-CBAM with correct forward pass and grad-cam target layer
- factory.py reads attn_layer from config and passes as block_args
- eval.py accepts --checkpoint-dir override for experiment-specific evaluation
- benchmark.py can orchestrate the full experiment grid (verified by --help and import)
- benchmark.py generates comparison.csv after experiment runs
- Existing B0 checkpoints will be backed up before experiments run
- Makefile has benchmark target
</success_criteria>

<output>
After completion, create `.planning/phases/09-model-improvement/09-01-SUMMARY.md`
</output>
