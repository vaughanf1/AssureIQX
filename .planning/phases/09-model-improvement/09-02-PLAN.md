---
phase: 09-model-improvement
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - configs/default.yaml
  - results/experiments/
  - docs/model_card.md
  - docs/poc_report.md
autonomous: false

must_haves:
  truths:
    - "All 4 experiments (E1-E4) have been trained on both split strategies (8 total training runs)"
    - "Each experiment has evaluation results (metrics_summary.json, classification_report.json) in results/experiments/{experiment_id}/"
    - "A comparison table shows accuracy, macro AUC, and malignant sensitivity across all experiments and both splits"
    - "The best-performing model is identified and its checkpoint is set as the default"
    - "Model card and PoC report are updated with improved results (if any experiment exceeds baseline)"
  artifacts:
    - path: "results/experiments/"
      provides: "Per-experiment results directories with metrics and checkpoints"
    - path: "results/experiments/comparison.csv"
      provides: "Cross-experiment comparison table"
    - path: "configs/default.yaml"
      provides: "Updated config pointing to best model architecture"
  key_links:
    - from: "scripts/benchmark.py"
      to: "results/experiments/"
      via: "benchmark execution creates per-experiment result directories"
      pattern: "results/experiments"
---

<objective>
Run the full experiment grid (4 architectures x 2 splits = 8 training runs), compare results across experiments, select the best model, and update documentation with improved metrics.

Purpose: Determine which architecture + annotation combination best improves classification accuracy, closing the gap with the BTXRD paper baseline.

Output: Trained models for all experiments, comparison table, updated default config, and revised model card/PoC report reflecting the best results.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-model-improvement/09-RESEARCH.md
@.planning/phases/09-model-improvement/09-01-SUMMARY.md
@scripts/benchmark.py
@configs/default.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run full experiment grid via benchmark.py</name>
  <files>results/experiments/, checkpoints/</files>
  <action>
Run the full benchmark:

```bash
python scripts/benchmark.py --config configs/default.yaml --experiments all
```

This will execute all 4 experiments (E1-E4) on both split strategies (stratified + center):

| ID | Backbone | Image Size | Batch Size | Annotations | Splits |
|----|----------|-----------|------------|-------------|--------|
| E1 | efficientnet_b0 | 224 | 32 | No | Both |
| E2 | efficientnet_b0 | 224 | 32 | Yes (ROI dim) | Both |
| E3 | efficientnet_b3 | 300 | 16 | Yes (ROI dim) | Both |
| E4 | resnet50+cbam | 224 | 16 | Yes (ROI dim) | Both |

**Expected duration:** Each training run takes 15-60 minutes on MPS depending on model size and image resolution. Total: 2-8 hours for all 8 runs.

**If any experiment fails (OOM, error):**
- For OOM: reduce batch_size by half and retry that experiment manually
- For code errors: fix the issue in the relevant source file and re-run just the failed experiment via `--experiments E3` (or whichever failed)

**After benchmark completes:**
- Verify `results/experiments/` has subdirectories for each experiment
- Each subdirectory should contain `stratified/` and `center_holdout/` with metrics files
- Verify checkpoints exist: `checkpoints/best_stratified_{experiment_id}.pt` and `checkpoints/best_center_{experiment_id}.pt`
  </action>
  <verify>
Check that all 8 experiment result directories exist and contain metrics:
```bash
for exp in E1_b0_baseline E2_b0_annotated E3_b3_annotated E4_resnet50_cbam; do
  for split in stratified center_holdout; do
    if [ -f "results/experiments/${exp}/${split}/metrics_summary.json" ]; then
      echo "OK: ${exp}/${split}"
    else
      echo "MISSING: ${exp}/${split}"
    fi
  done
done
```
All 8 should print "OK".
  </verify>
  <done>All 4 experiments trained on both splits with evaluation metrics saved to results/experiments/.</done>
</task>

<task type="auto">
  <name>Task 2: Generate comparison report and select best model</name>
  <files>results/experiments/comparison.csv, results/experiments/comparison.json</files>
  <action>
After Task 1 completes, create a comparison analysis:

1. Read `metrics_summary.json` from each experiment's results directory
2. Build a comparison table with columns:
   - Experiment ID
   - Backbone
   - Image Size
   - Annotations (yes/no)
   - Stratified Accuracy
   - Stratified Macro AUC
   - Stratified Malignant Sensitivity
   - Center-Holdout Accuracy
   - Center-Holdout Macro AUC
   - Center-Holdout Malignant Sensitivity
3. Save as `results/experiments/comparison.csv` and `results/experiments/comparison.json`
4. Determine the best experiment based on the primary criterion: **stratified accuracy > 75%**, with secondary criteria: center-holdout accuracy > 55%, malignant sensitivity > 70%
5. Print the comparison table to stdout with the winner highlighted

**If writing a script for this:** Create a small section at the end of benchmark.py or a standalone analysis in the same script that reads the metrics files and produces the comparison. If benchmark.py already handles this, just verify the output.

**Selection logic for "best model":**
- Primary: highest stratified accuracy
- Tiebreaker 1: highest malignant sensitivity (stratified)
- Tiebreaker 2: highest center-holdout accuracy
  </action>
  <verify>
`cat results/experiments/comparison.csv` shows all 4 experiments with metrics for both splits.
The best experiment is identified in the output.
  </verify>
  <done>Comparison table generated showing metrics across all experiments. Best model identified by stratified accuracy with malignant sensitivity as tiebreaker.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Full experiment grid results comparing 4 architectures on both split strategies:
- E1: EfficientNet-B0 baseline (no annotations)
- E2: EfficientNet-B0 + annotation-guided training
- E3: EfficientNet-B3 + annotation-guided training
- E4: ResNet-50-CBAM + annotation-guided training

Comparison table at `results/experiments/comparison.csv`.

Success criteria from research:
- Primary: Stratified accuracy > 75% (baseline: 67.9%)
- Secondary: Center-holdout accuracy > 55% (baseline: 47.2%)
- Stretch: Malignant sensitivity > 70% (baseline: 60.8%)
  </what-built>
  <how-to-verify>
1. Review `results/experiments/comparison.csv` -- does any experiment exceed the 75% stratified accuracy target?
2. Check malignant sensitivity -- has it improved from the 60.8% baseline?
3. Check center-holdout generalization -- has the gap narrowed?
4. Decide: which model should become the new default? Or keep B0 baseline if no significant improvement?
5. Decide: should the model card and PoC report be updated with new results?
  </how-to-verify>
  <resume-signal>
Respond with:
- "best: E{N}" to set experiment N as the new default (updates config + docs)
- "keep baseline" to keep current B0 as default (no config/doc changes)
- Or provide specific instructions
  </resume-signal>
</task>

<task type="auto">
  <name>Task 3: Update config and documentation with best model results</name>
  <files>configs/default.yaml, docs/model_card.md, docs/poc_report.md</files>
  <action>
Based on the checkpoint decision:

**If a new best model is selected (e.g., "best: E3"):**

1. Update `configs/default.yaml`:
   - `model.backbone` to the winning backbone
   - `data.image_size` to the winning image size
   - `training.batch_size` to the winning batch size
   - `model.dropout` to the winning dropout
   - `model.attn_layer` if the winner uses CBAM (add this key; omit if not needed)
   - `inference.default_checkpoint` to point to the winning checkpoint

2. Copy the winning checkpoint to the standard names:
   - `checkpoints/best_stratified.pt` (from `best_stratified_{experiment_id}.pt`)
   - `checkpoints/best_center.pt` (from `best_center_{experiment_id}.pt`)

3. Update `docs/model_card.md`:
   - Update architecture section with new backbone details
   - Update performance metrics table with new results from comparison
   - Add a "Model Improvement (Phase 9)" section documenting:
     - Experiment grid and methodology
     - Results comparison table
     - Why this architecture was selected
     - Improvement over B0 baseline (delta for each metric)

4. Update `docs/poc_report.md`:
   - Update results section with new metrics
   - Add Phase 9 experiment results to methodology section
   - Update next steps to reflect what was achieved

**If "keep baseline" is chosen:**
1. Restore `configs/default.yaml` to B0 settings (backbone=efficientnet_b0, image_size=224, batch_size=32, dropout=0.2, remove annotations_dir if it was changed)
2. Add a brief note to model_card.md documenting the experiment grid and that B0 remained the best option
3. No checkpoint changes needed (B0 originals are in backup)
  </action>
  <verify>
- `python -c "from src.utils.config import load_config; c = load_config('configs/default.yaml'); print(c['model']['backbone'])"` shows the selected backbone
- `cat docs/model_card.md | grep -c 'Phase 9'` returns >= 1 (Phase 9 documented)
- The default checkpoint referenced in config exists: `ls $(python -c "from src.utils.config import load_config; c = load_config('configs/default.yaml'); print(c['inference']['default_checkpoint'])")`
  </verify>
  <done>Config updated to best model, checkpoint paths correct, model card and PoC report updated with Phase 9 experiment results and improved metrics.</done>
</task>

</tasks>

<verification>
1. `results/experiments/` contains 4 experiment subdirectories (E1-E4), each with stratified/ and center_holdout/ metrics
2. `results/experiments/comparison.csv` exists with metrics for all experiments
3. `configs/default.yaml` points to the best-performing model architecture
4. `docs/model_card.md` documents Phase 9 experiment results
5. The winning checkpoint is accessible via the standard path in config
</verification>

<success_criteria>
- All 4 experiments completed training on both splits (8 total runs)
- Comparison table generated with accuracy, AUC, and sensitivity metrics
- Best model selected and set as default in config
- Documentation updated with Phase 9 results
- At least one experiment shows improvement over B0 baseline (67.9% stratified accuracy)
</success_criteria>

<output>
After completion, create `.planning/phases/09-model-improvement/09-02-SUMMARY.md`
</output>
