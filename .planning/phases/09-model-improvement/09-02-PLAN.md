---
phase: 09-model-improvement
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - configs/default.yaml
  - results/experiments/
  - results/gradcam/
  - docs/model_card.md
  - docs/poc_report.md
autonomous: false

must_haves:
  truths:
    - "All 4 experiments (E1-E4) have been trained on both split strategies (8 total training runs)"
    - "Each experiment has evaluation results (metrics_summary.json, classification_report.json) in results/experiments/{experiment_id}/"
    - "A comparison table shows accuracy, macro AUC, and malignant sensitivity across all experiments and both splits"
    - "The best-performing model is identified and its checkpoint is set as the default"
    - "Model card and PoC report are updated with experiment results (regardless of whether any experiment exceeds baseline)"
    - "Grad-CAM visualizations are regenerated using the best experiment's checkpoint"
    - "At least one experiment improves over 67.9% stratified accuracy baseline, OR all experiment results are honestly documented showing no improvement"
  artifacts:
    - path: "results/experiments/"
      provides: "Per-experiment results directories with metrics and checkpoints"
    - path: "results/experiments/comparison.csv"
      provides: "Cross-experiment comparison table"
    - path: "results/gradcam/"
      provides: "Grad-CAM visualizations from the best experiment's checkpoint"
    - path: "configs/default.yaml"
      provides: "Updated config pointing to best model architecture"
  key_links:
    - from: "scripts/benchmark.py"
      to: "results/experiments/"
      via: "benchmark execution creates per-experiment result directories"
      pattern: "results/experiments"
    - from: "scripts/gradcam.py"
      to: "results/gradcam/"
      via: "gradcam regeneration using best checkpoint"
      pattern: "gradcam"
---

<objective>
Run the full experiment grid (4 architectures x 2 splits = 8 training runs), compare results across experiments, select the best model, and update documentation with improved metrics.

Purpose: Determine which architecture + annotation combination best improves classification accuracy, closing the gap with the BTXRD paper baseline.

Output: Trained models for all experiments, comparison table, updated default config, and revised model card/PoC report reflecting the best results.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-model-improvement/09-RESEARCH.md
@.planning/phases/09-model-improvement/09-01-SUMMARY.md
@scripts/benchmark.py
@configs/default.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run experiment grid via benchmark.py with per-experiment verification</name>
  <files>results/experiments/, checkpoints/</files>
  <action>
Run experiments sequentially with intermediate verification after each one. Do NOT run all experiments in a single unattended command.

**Step 1: Run E1 and E2 (EfficientNet-B0 experiments)**
```bash
python scripts/benchmark.py --config configs/default.yaml --experiments E1_b0_baseline,E2_b0_annotated
```

After completion, verify both experiments produced results:
```bash
ls results/experiments/E1_b0_baseline/stratified/metrics_summary.json
ls results/experiments/E1_b0_baseline/center_holdout/metrics_summary.json
ls results/experiments/E2_b0_annotated/stratified/metrics_summary.json
ls results/experiments/E2_b0_annotated/center_holdout/metrics_summary.json
```
All 4 files must exist. If any are missing, diagnose and fix before proceeding.

**Step 2: Run E3 (EfficientNet-B3)**
```bash
python scripts/benchmark.py --config configs/default.yaml --experiments E3_b3_annotated
```

Verify:
```bash
ls results/experiments/E3_b3_annotated/stratified/metrics_summary.json
ls results/experiments/E3_b3_annotated/center_holdout/metrics_summary.json
```

**Step 3: Run E4 (ResNet-50-CBAM)**
```bash
python scripts/benchmark.py --config configs/default.yaml --experiments E4_resnet50_cbam
```

Verify:
```bash
ls results/experiments/E4_resnet50_cbam/stratified/metrics_summary.json
ls results/experiments/E4_resnet50_cbam/center_holdout/metrics_summary.json
```

**If any experiment fails (OOM, error):**
- For OOM: reduce batch_size by half and retry that experiment: `python scripts/benchmark.py --experiments E3_b3_annotated --override training.batch_size=8`
- For code errors: fix the issue in the relevant source file and re-run just the failed experiment

**Expected duration:** ~30-60 min per experiment (2 splits each). Total: 2-4 hours.
  </action>
  <verify>
Check that all 8 experiment result directories exist and contain metrics:
```bash
for exp in E1_b0_baseline E2_b0_annotated E3_b3_annotated E4_resnet50_cbam; do
  for split in stratified center_holdout; do
    if [ -f "results/experiments/${exp}/${split}/metrics_summary.json" ]; then
      echo "OK: ${exp}/${split}"
    else
      echo "MISSING: ${exp}/${split}"
    fi
  done
done
```
All 8 should print "OK".

Additionally verify comparison table was generated:
```bash
ls results/experiments/comparison.csv
```
  </verify>
  <done>All 4 experiments trained on both splits with evaluation metrics and comparison.csv saved to results/experiments/.</done>
</task>

<task type="auto">
  <name>Task 2: Verify comparison table and identify best model</name>
  <files>results/experiments/comparison.csv</files>
  <action>
The comparison table (`results/experiments/comparison.csv`) is generated by benchmark.py (built in 09-01). This task verifies its completeness and correctness.

1. Read and display `results/experiments/comparison.csv` -- confirm it has rows for all 4 experiments with metrics for both splits
2. Verify the table includes columns: Experiment ID, Backbone, Image Size, Annotations, Stratified Accuracy, Stratified Macro AUC, Stratified Malignant Sensitivity, Center-Holdout Accuracy, Center-Holdout Macro AUC, Center-Holdout Malignant Sensitivity
3. Cross-check one experiment's metrics in the table against its raw `metrics_summary.json` to ensure accuracy
4. Confirm the best experiment identified by benchmark.py matches the selection criteria:
   - Primary: highest stratified accuracy
   - Tiebreaker 1: highest malignant sensitivity (stratified)
   - Tiebreaker 2: highest center-holdout accuracy
5. Print the comparison table and the winning experiment to stdout
  </action>
  <verify>
`cat results/experiments/comparison.csv` shows all 4 experiments with metrics for both splits.
The best experiment is identified and printed.
  </verify>
  <done>Comparison table verified as complete and accurate. Best model identified by stratified accuracy with malignant sensitivity as tiebreaker.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Full experiment grid results comparing 4 architectures on both split strategies:
- E1: EfficientNet-B0 baseline (no annotations)
- E2: EfficientNet-B0 + annotation-guided training
- E3: EfficientNet-B3 + annotation-guided training
- E4: ResNet-50-CBAM + annotation-guided training

Comparison table at `results/experiments/comparison.csv`.

Success criteria from research:
- Primary: Stratified accuracy > 75% (baseline: 67.9%)
- Secondary: Center-holdout accuracy > 55% (baseline: 47.2%)
- Stretch: Malignant sensitivity > 70% (baseline: 60.8%)
  </what-built>
  <how-to-verify>
1. Review `results/experiments/comparison.csv` -- does any experiment exceed the 75% stratified accuracy target?
2. Check malignant sensitivity -- has it improved from the 60.8% baseline?
3. Check center-holdout generalization -- has the gap narrowed?
4. Decide: which model should become the new default? Or keep B0 baseline if no significant improvement?
5. Decide: should the model card and PoC report be updated with new results?
  </how-to-verify>
  <resume-signal>
Respond with:
- "best: E{N}" to set experiment N as the new default (updates config + docs)
- "keep baseline" to keep current B0 as default (no config/doc changes)
- Or provide specific instructions
  </resume-signal>
</task>

<task type="auto">
  <name>Task 3: Update config and documentation with best model results</name>
  <files>configs/default.yaml, docs/model_card.md, docs/poc_report.md</files>
  <action>
Based on the checkpoint decision:

**If a new best model is selected (e.g., "best: E3"):**

1. Update `configs/default.yaml`:
   - `model.backbone` to the winning backbone
   - `data.image_size` to the winning image size
   - `training.batch_size` to the winning batch size
   - `model.dropout` to the winning dropout
   - `model.attn_layer` if the winner uses CBAM (add this key; omit if not needed)
   - `inference.default_checkpoint` to point to the winning checkpoint

2. Copy the winning checkpoint to the standard names:
   - `checkpoints/best_stratified.pt` (from `best_stratified_{experiment_id}.pt`)
   - `checkpoints/best_center.pt` (from `best_center_{experiment_id}.pt`)

3. Update `docs/model_card.md`:
   - Update architecture section with new backbone details
   - Update performance metrics table with new results from comparison
   - Add a "Model Improvement (Phase 9)" section documenting:
     - Experiment grid and methodology
     - Results comparison table
     - Why this architecture was selected
     - Improvement over B0 baseline (delta for each metric)

4. Update `docs/poc_report.md`:
   - Update results section with new metrics
   - Add Phase 9 experiment results to methodology section
   - Update next steps to reflect what was achieved

**If "keep baseline" is chosen:**
1. Restore `configs/default.yaml` to B0 settings (backbone=efficientnet_b0, image_size=224, batch_size=32, dropout=0.2, remove annotations_dir if it was changed)
2. Add a brief note to model_card.md documenting the experiment grid and that B0 remained the best option
3. No checkpoint changes needed (B0 originals are in backup)
  </action>
  <verify>
- `python -c "from src.utils.config import load_config; c = load_config('configs/default.yaml'); print(c['model']['backbone'])"` shows the selected backbone
- `cat docs/model_card.md | grep -c 'Phase 9'` returns >= 1 (Phase 9 documented)
- The default checkpoint referenced in config exists: `ls $(python -c "from src.utils.config import load_config; c = load_config('configs/default.yaml'); print(c['inference']['default_checkpoint'])")`
  </verify>
  <done>Config updated to best model, checkpoint paths correct, model card and PoC report updated with Phase 9 experiment results and improved metrics.</done>
</task>

<task type="auto">
  <name>Task 4: Regenerate Grad-CAM visualizations with best model checkpoint</name>
  <files>results/gradcam/</files>
  <action>
Run Grad-CAM visualization using the best experiment's checkpoint to complete the full evaluation pipeline.

1. After Task 3 sets the winning checkpoint as the default, run:
```bash
python scripts/gradcam.py --config configs/default.yaml
```

This uses `inference.default_checkpoint` from the updated config, which now points to the best experiment's checkpoint (copied to the standard name in Task 3).

2. Verify that Grad-CAM images are generated in `results/gradcam/`:
   - Should contain per-class Grad-CAM visualizations (Normal, Benign, Malignant)
   - Images should show attention heatmaps from the winning model architecture

3. If the winning model is a different architecture (e.g., ResNet-50 instead of EfficientNet-B0), the `gradcam_target_layer` property (updated in 09-01) handles selecting the correct layer automatically.

**If "keep baseline" was chosen in the checkpoint:** Run gradcam.py with the existing B0 checkpoint (no config changes needed since it still points to B0). The Grad-CAM output will match the existing results but confirms the pipeline still works end-to-end.
  </action>
  <verify>
```bash
ls results/gradcam/*.png | wc -l
```
Should show >= 3 Grad-CAM images (one per class minimum).

Verify images are non-empty:
```bash
for f in results/gradcam/*.png; do
  size=$(stat -f%z "$f" 2>/dev/null || stat -c%s "$f" 2>/dev/null)
  if [ "$size" -gt 1000 ]; then echo "OK: $f"; else echo "EMPTY: $f"; fi
done
```
  </verify>
  <done>Grad-CAM visualizations regenerated using the best experiment's checkpoint, completing the full evaluation pipeline for the improved model.</done>
</task>

</tasks>

<verification>
1. `results/experiments/` contains 4 experiment subdirectories (E1-E4), each with stratified/ and center_holdout/ metrics
2. `results/experiments/comparison.csv` exists with metrics for all experiments
3. `configs/default.yaml` points to the best-performing model architecture
4. `docs/model_card.md` documents Phase 9 experiment results
5. The winning checkpoint is accessible via the standard path in config
6. `results/gradcam/` contains Grad-CAM visualizations generated from the best model's checkpoint
</verification>

<success_criteria>
- All 4 experiments completed training on both splits (8 total runs)
- Comparison table generated with accuracy, AUC, and sensitivity metrics
- Best model selected and set as default in config
- Documentation updated with Phase 9 results (whether improved or not)
- Grad-CAM visualizations regenerated with the best model's checkpoint
- Either: at least one experiment improves over B0 baseline (67.9% stratified accuracy), OR: all results honestly documented showing no improvement with analysis of why
</success_criteria>

<output>
After completion, create `.planning/phases/09-model-improvement/09-02-SUMMARY.md`
</output>
