---
phase: 06-explainability-and-inference
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/explainability/gradcam.py
  - scripts/gradcam.py
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/gradcam.py` generates Grad-CAM heatmap overlays for TP, FP, FN per class"
    - "3-5 curated examples per category (TP, FP, FN) per class are saved as image grids in results/gradcam/"
    - "Annotation comparison panel shows Grad-CAM attention vs LabelMe tumor annotation masks with IoU scores"
    - "Normal class TP examples show attention patterns without IoU (no annotations exist for Normal)"
  artifacts:
    - path: "src/explainability/gradcam.py"
      provides: "Reusable Grad-CAM functions: generate_gradcam, denormalize_tensor, create_overlay, load_annotation_mask, compute_cam_iou, build_gallery_grid"
      min_lines: 150
    - path: "results/gradcam/gallery_Normal.png"
      provides: "Curated TP/FP/FN gallery grid for Normal class"
    - path: "results/gradcam/gallery_Benign.png"
      provides: "Curated TP/FP/FN gallery grid for Benign class"
    - path: "results/gradcam/gallery_Malignant.png"
      provides: "Curated TP/FP/FN gallery grid for Malignant class"
    - path: "results/gradcam/annotation_comparison.png"
      provides: "Side-by-side panels: original, annotation mask, Grad-CAM heatmap, overlay for tumor images"
    - path: "results/gradcam/annotation_report.json"
      provides: "IoU scores per image and summary statistics for annotation comparison"
  key_links:
    - from: "scripts/gradcam.py"
      to: "src/explainability/gradcam.py"
      via: "imports generate_gradcam, denormalize_tensor, load_annotation_mask, compute_cam_iou, build_gallery_grid"
      pattern: "from src\\.explainability\\.gradcam import"
    - from: "scripts/gradcam.py"
      to: "src/models/factory.py"
      via: "load_checkpoint, create_model, get_device (same pattern as eval.py)"
      pattern: "from src\\.models\\.factory import"
    - from: "src/explainability/gradcam.py"
      to: "pytorch_grad_cam"
      via: "GradCAM context manager, ClassifierOutputTarget, show_cam_on_image"
      pattern: "from pytorch_grad_cam import GradCAM"
---

<objective>
Implement the Grad-CAM explainability module and curated gallery script (EXPL-01, EXPL-02, EXPL-03).

Purpose: Clinicians need to inspect where the model attends when making predictions. A curated gallery of TP/FP/FN examples per class reveals both correct attention patterns and interpretable failure modes. The annotation comparison quantifies whether the model attends to annotated tumor regions.

Output:
- `src/explainability/gradcam.py` -- reusable functions for Grad-CAM generation, overlay creation, annotation mask loading, IoU computation, and image grid assembly
- `scripts/gradcam.py` -- orchestration script that loads the stratified checkpoint, runs inference on the stratified test set, selects TP/FP/FN examples, generates per-class gallery grids, and produces the annotation comparison panel with IoU report
- Gallery PNGs and annotation report JSON in `results/gradcam/`
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-explainability-and-inference/06-RESEARCH.md

# Key source files to reference for patterns
@src/models/classifier.py       -- BTXRDClassifier with gradcam_target_layer property
@src/models/factory.py          -- create_model, load_checkpoint, get_device
@src/data/dataset.py            -- BTXRDDataset, create_dataloader, CLASS_TO_IDX
@src/data/transforms.py         -- get_test_transforms, IMAGENET_MEAN, IMAGENET_STD
@src/evaluation/metrics.py      -- run_inference function (reuse pattern)
@scripts/eval.py                -- checkpoint loading + inference orchestration pattern
@configs/default.yaml           -- gradcam config section (examples_per_class: 3, target_layer: auto)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement src/explainability/gradcam.py utility module</name>
  <files>src/explainability/gradcam.py</files>
  <action>
Replace the placeholder docstring in `src/explainability/gradcam.py` with a complete module providing these functions:

1. **`denormalize_tensor(tensor)`** -- Convert a normalized CHW tensor to HWC float32 [0,1] numpy array for visualization. Use IMAGENET_MEAN and IMAGENET_STD from `src.data.transforms`. Apply `np.clip(img, 0, 1)` to prevent overflow.

2. **`generate_gradcam(model, input_tensor, target_class_idx)`** -- Generate a Grad-CAM heatmap for a single input tensor (1, C, H, W) targeting a specific class. Use `pytorch_grad_cam.GradCAM` with context manager pattern. Target layer: `model.gradcam_target_layer`. Return grayscale heatmap (H, W) float32 in [0, 1]. Warn if heatmap max is 0.0 (degenerate case). Do NOT wrap in `torch.no_grad()` -- GradCAM needs gradients. But model MUST be in eval mode.

3. **`create_overlay(rgb_img, heatmap)`** -- Overlay heatmap onto RGB image using `show_cam_on_image(rgb_img, heatmap, use_rgb=True)`. Input: rgb_img is HWC float32 [0,1], heatmap is HW float32 [0,1]. Return: HWC uint8 [0,255].

4. **`load_annotation_mask(ann_path, target_size=224)`** -- Load LabelMe JSON annotation, rasterize shapes (polygon via `cv2.fillPoly`, rectangle via `cv2.rectangle`) onto a binary mask at original resolution, then resize to `target_size x target_size` with `cv2.INTER_NEAREST`. Return binary uint8 mask.

5. **`compute_cam_iou(cam_heatmap, annotation_mask, threshold=0.5)`** -- Binarize the heatmap at threshold, compute IoU (intersection over union) with the annotation mask. Return float IoU score (0.0 if union is 0).

6. **`select_examples(y_true, y_pred, y_prob, class_idx, k=3)`** -- Select top-k TP, FP, FN indices for a given class. TP sorted by highest confidence (most confident correct), FP sorted by highest confidence (most confident mistakes), FN sorted by lowest confidence for true class (model was most wrong). Use `min(k, len(available))` to handle classes with fewer than k examples. Return tuple of (tp_indices, fp_indices, fn_indices) as numpy arrays.

7. **`build_gallery_grid(images, titles, rows, cols, output_path, suptitle="")`** -- Create a matplotlib figure with `plt.subplots(rows, cols)`. Show each image with its title (fontsize=9). Hide unused subplots. Save at DPI 150 with `bbox_inches="tight"`. Close figure to prevent memory leak. Use `matplotlib.use("Agg")` at module level.

Imports needed: `numpy`, `cv2`, `json`, `logging`, `pathlib.Path`, `matplotlib` (with Agg backend), `matplotlib.pyplot`, `pytorch_grad_cam.GradCAM`, `pytorch_grad_cam.utils.model_targets.ClassifierOutputTarget`, `pytorch_grad_cam.utils.image.show_cam_on_image`, and `src.data.transforms.IMAGENET_MEAN`/`IMAGENET_STD`.

Do NOT import torch at module level for the functions that don't need it -- keep imports clean.
  </action>
  <verify>
Run: `python -c "from src.explainability.gradcam import denormalize_tensor, generate_gradcam, create_overlay, load_annotation_mask, compute_cam_iou, select_examples, build_gallery_grid; print('All imports OK')"`
Confirm all 7 functions are importable without errors.
  </verify>
  <done>
`src/explainability/gradcam.py` contains 7 working functions covering Grad-CAM generation, overlay creation, annotation mask loading, IoU computation, example selection, and gallery grid assembly. All functions follow the patterns verified in 06-RESEARCH.md. No new dependencies required.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement scripts/gradcam.py gallery orchestration</name>
  <files>scripts/gradcam.py</files>
  <action>
Replace the `NotImplementedError` stub in `scripts/gradcam.py` with a full orchestration script. Keep the existing argparse skeleton (--config, --override). Follow the pattern from `scripts/eval.py` for checkpoint loading, dataset creation, and inference.

**Main flow:**

1. **Setup:** Load config, set seed, configure logging (same pattern as eval.py). Resolve device via `get_device`. Create output directory `results/gradcam/`.

2. **Load model:** Use stratified checkpoint by default (`cfg["inference"]["default_checkpoint"]`). Load with `load_checkpoint`, `create_model`, `model.load_state_dict`, `model.to(device)`, `model.eval()`.

3. **Load test dataset:** Create `BTXRDDataset` from `data/splits/stratified_test.csv` with `get_test_transforms`. Create DataLoader with `shuffle=False`.

4. **Run inference:** Use `run_inference(model, test_loader, device)` from `src.evaluation.metrics` to get `y_true, y_pred, y_prob`. Log class distribution and accuracy.

5. **Generate per-class galleries (EXPL-01 + EXPL-02):**
   For each class (Normal=0, Benign=1, Malignant=2):
   - Call `select_examples(y_true, y_pred, y_prob, class_idx, k=cfg["gradcam"]["examples_per_class"])` to get TP, FP, FN indices.
   - For each selected index, retrieve the image tensor from the dataset (via `dataset[idx]`), generate Grad-CAM targeting the PREDICTED class, denormalize, create overlay.
   - Build a grid with 3 rows (TP, FP, FN) and k columns. Each cell shows the Grad-CAM overlay. Title format: `"TP: {true_class} -> {pred_class} ({prob:.2f})"` (similarly for FP, FN).
   - Save as `results/gradcam/gallery_{class_name}.png` using `build_gallery_grid`.
   - Log how many TP/FP/FN examples were found per class.

6. **Annotation comparison (EXPL-03):**
   - Select up to 5 correctly classified tumor images (Benign TP + Malignant TP) that have LabelMe annotations.
   - For each: load annotation mask from `data_raw/annotations/{image_id}.json` (replace image extension with `.json`), generate Grad-CAM, compute IoU.
   - Create a 4-panel comparison figure per image: [Original | Annotation Mask | Grad-CAM Heatmap | Overlay with IoU]. Use `plt.subplots(1, 4)`.
   - Combine all comparison panels into one figure: `results/gradcam/annotation_comparison.png`.
   - Save IoU results to `results/gradcam/annotation_report.json` with per-image IoU scores, mean IoU, and a qualitative summary noting whether tumor images show focal attention on annotated regions.
   - For the annotation path, strip the image extension and append `.json`. The image_id in the dataset has extensions like `.jpeg` or `.jpg`.

**Critical details:**
- Do NOT wrap GradCAM calls in `torch.no_grad()`. The GradCAM context manager handles its own gradient state.
- DO use `torch.no_grad()` for the initial inference pass via `run_inference`.
- The dataset `__getitem__` returns `(tensor, label_idx)`. The tensor is already normalized. To get the raw image for overlay, denormalize the tensor.
- Save overlay images with `PIL.Image.fromarray(overlay)` (RGB), NOT `cv2.imwrite` (BGR).
- Handle edge case: if fewer than k examples exist for any category, use all available and log a warning.
- The `generate_gradcam` function needs the input tensor on the correct device. Ensure `.unsqueeze(0).to(device)` for single images from the dataset.
  </action>
  <verify>
Run: `python scripts/gradcam.py --config configs/default.yaml`
Confirm output:
1. `results/gradcam/gallery_Normal.png` exists and shows a grid of heatmap overlays
2. `results/gradcam/gallery_Benign.png` exists
3. `results/gradcam/gallery_Malignant.png` exists
4. `results/gradcam/annotation_comparison.png` exists with 4-panel comparisons
5. `results/gradcam/annotation_report.json` exists with IoU scores and mean IoU
6. Script completes without errors and logs summary statistics
  </verify>
  <done>
Running `make gradcam` produces per-class gallery grids (TP/FP/FN with 3 examples each) and an annotation comparison panel with IoU scores. All outputs are in `results/gradcam/`. The gallery demonstrates where the model attends for correct predictions and failure modes. The annotation comparison quantifies overlap between Grad-CAM attention and expert-annotated tumor regions.
  </done>
</task>

</tasks>

<verification>
1. `python scripts/gradcam.py --config configs/default.yaml` completes without errors
2. `ls results/gradcam/` shows: gallery_Normal.png, gallery_Benign.png, gallery_Malignant.png, annotation_comparison.png, annotation_report.json
3. Gallery images contain 3 rows (TP, FP, FN) x k columns with titled subplots
4. `python -c "import json; r=json.load(open('results/gradcam/annotation_report.json')); print(f'Mean IoU: {r[\"mean_iou\"]:.3f}, N images: {len(r[\"per_image\"])}')"` shows valid IoU scores
5. `make gradcam` executes successfully (Makefile target already exists)
</verification>

<success_criteria>
- EXPL-01: Grad-CAM heatmap generation works for all 3 classes using model.gradcam_target_layer
- EXPL-02: Curated gallery with 3-5 TP/FP/FN examples per class saved as image grids
- EXPL-03: Annotation comparison with IoU scores for tumor images (Benign + Malignant TPs)
- All functions in src/explainability/gradcam.py are reusable by scripts/infer.py (Plan 06-02)
</success_criteria>

<output>
After completion, create `.planning/phases/06-explainability-and-inference/06-01-SUMMARY.md`
</output>
