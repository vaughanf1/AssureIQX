# Phase 7: Documentation and Reports - Research

**Researched:** 2026-02-20
**Domain:** ML model documentation (model cards, PoC reports, README), clinical framing of classification results
**Confidence:** HIGH

## Summary

Phase 7 is a pure documentation phase -- no new code, no new models, no new libraries. The deliverables are three Markdown files (`docs/model_card.md`, `docs/poc_report.md`, updated `README.md`) that synthesize all results from Phases 1-6 into publication-quality documentation. All data needed to populate these documents already exists in the `results/` directory as JSON files, CSV files, and PNG figures.

The model card follows the Mitchell et al. (2019) framework with 9 sections: Model Details, Intended Use, Factors, Metrics, Evaluation Data, Training Data, Quantitative Analyses, Ethical Considerations, and Caveats & Recommendations. The PoC report follows a standard scientific report structure: Executive Summary, Methods, Results, Grad-CAM Findings, Limitations, Clinical Relevance, and Next Steps. The README update extends the existing 104-line README with complete CLI examples, data download instructions, and end-to-end reproduction commands.

Key challenge: honestly framing results that are modest (macro AUC 0.846 stratified, 0.627 center-holdout) with wide confidence intervals (Malignant sensitivity CI 0.474-0.743 stratified) and low Grad-CAM alignment (mean IoU 0.070). The documentation must clearly convey that this is a feasibility demonstration, not a clinical tool, while still articulating what was learned and what the results mean for future work.

**Primary recommendation:** Structure as two plans: (1) model card and PoC report (DOCS-03, DOCS-04), which are the core documentation deliverables, (2) README finalization and end-to-end reproduction verification. All content is manually authored Markdown populated with concrete numbers from `results/` JSON files. No scripts or code needed -- this is writing, not engineering.

## Standard Stack

### Core

| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| Markdown | N/A | Document format for all three deliverables | Standard for GitHub-hosted ML projects; renders natively on GitHub, supports tables, images, code blocks |
| JSON (results data) | N/A | Source of all metrics, CIs, comparison data | Already generated by Phase 5 scripts; authoritative source of truth for numbers |

### Supporting

| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| None | - | - | No code dependencies for this phase |

### Alternatives Considered

| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Markdown model card | HuggingFace Model Card YAML metadata | HuggingFace format useful for Hub deployment but overkill for a PoC report; plain Markdown is more accessible and matches the project convention |
| Manual Markdown report | Jupyter notebook report | Notebooks mix code and prose; docs/ files should be standalone readable documents without needing a kernel |
| Static Markdown tables | Auto-generated report script | A script that reads JSON and generates Markdown would be more maintainable but adds complexity for a one-time PoC; manual tables with concrete numbers are simpler and verifiable |

**Installation:** No new packages needed. This is a documentation-only phase.

## Architecture Patterns

### Recommended Document Structure

```
docs/
  model_card.md           # Mitchell et al. (2019) format model card
  poc_report.md           # Comprehensive PoC report with clinical framing
  data_audit_report.md    # Already exists (Phase 2)
  dataset_spec.md         # Already exists (Phase 2)
  figures/                # Already exists (Phase 2) -- audit figures

README.md                 # Updated top-level README with full reproduction instructions

results/                  # Source data for documentation (all already exists)
  stratified/
    metrics_summary.json
    bootstrap_ci.json
    classification_report.json
    training_log.csv
    *.png (ROC, PR, confusion matrix, loss curves)
  center_holdout/
    metrics_summary.json
    bootstrap_ci.json
    classification_report.json
    training_log.csv
    *.png
  comparison_table.json
  comparison_table.csv
  gradcam/
    annotation_report.json
    gallery_*.png
    annotation_comparison.png
```

### Pattern 1: Mitchell et al. (2019) Model Card Structure

**What:** The standard 9-section model card format for transparent ML model reporting.
**When to use:** For `docs/model_card.md` (DOCS-03).
**Source:** Mitchell et al., "Model Cards for Model Reporting," FAccT 2019 (https://arxiv.org/abs/1810.03993)

```markdown
# Model Card: AssureXRay Bone Tumor Classifier

## Model Details
- Developer, date, version, model type (EfficientNet-B0), framework (PyTorch)
- Architecture details: backbone, num_params, input size, output
- Training methodology: transfer learning, weighted loss, early stopping
- Citation/license information

## Intended Use
- Primary: Research proof-of-concept for bone tumor classification feasibility
- Users: ML researchers, medical imaging researchers
- Out of scope: Clinical diagnosis, treatment planning, patient triage
- NOT FOR CLINICAL USE disclaimer (prominent)

## Factors
- Relevant factors: medical center (source bias), anatomical site, tumor subtype
- Demographic factors: age, gender (available but not analyzed for subgroup fairness)
- Environmental: radiograph quality varies by center, imaging equipment differences

## Metrics
- Why these metrics: AUC for discrimination, sensitivity for clinical relevance
- Malignant sensitivity as headline metric (missing cancer is worse than false alarm)
- Bootstrap CIs for uncertainty quantification

## Evaluation Data
- Stratified test set: 564 images (282 Normal, 231 Benign, 51 Malignant)
- Center-holdout test set: 808 images (286 Normal, 415 Benign, 107 Malignant)
- Preprocessing: Resize 224x224, ImageNet normalization (no augmentation)

## Training Data
- BTXRD dataset: 3,746 images from 3 centers
- Stratified train: 2,622 images; Center-holdout train: 2,499 images
- Augmentation: CLAHE, horizontal flip, +/-15 degree rotation
- Class-weighted loss: inverse frequency weights

## Quantitative Analyses
- Performance tables with both splits
- Bootstrap 95% CIs
- Comparison against BTXRD paper baseline (with 7 caveats)
- Generalization gap quantification

## Ethical Considerations
- Dataset license: CC BY-NC-ND 4.0 (non-commercial only)
- Labels based on radiological assessment, not all pathology-confirmed
- Potential for data leakage (same-lesion multi-angle images, no patient_id)
- Model should never be used for clinical decisions without validation
- Class imbalance means Malignant predictions are less reliable

## Caveats and Recommendations
- PoC-scale dataset (3,746 images) limits generalizability
- Single architecture evaluated (no ensemble, no architecture search)
- Grad-CAM IoU of 0.070 suggests model may use non-obvious features
- Center-holdout gap (-0.219 AUC) highlights limited cross-center generalization
```

### Pattern 2: PoC Report Structure

**What:** Scientific-report-style PoC document with clinical framing.
**When to use:** For `docs/poc_report.md` (DOCS-04).

```markdown
# AssureXRay: Proof-of-Concept Report

## Executive Summary
- 3-5 paragraph summary of the entire project
- Key finding: feasibility demonstrated for stratified split (AUC 0.846)
- Key limitation: center-holdout generalization gap (AUC 0.627)
- Conclusion: further data and multi-center validation needed

## 1. Introduction
- Problem: bone tumor classification from radiographs
- Objective: establish feasibility baseline
- Dataset: BTXRD (3,746 images, 3 classes, 3 centers)

## 2. Methods
### 2.1 Data
- BTXRD dataset description, class distribution, center breakdown
- Label derivation logic
### 2.2 Split Strategies
- Stratified: 70/15/15, class-stratified, duplicate-aware
- Center-holdout: Center 1 train/val, Centers 2+3 test
- Leakage risk documentation
### 2.3 Model Architecture
- EfficientNet-B0, ImageNet pretrained, 3-class head
- 4,011,391 parameters, full fine-tuning
### 2.4 Training
- Weighted cross-entropy, cosine LR scheduler
- Early stopping patience 7, batch size 32
- Augmentation pipeline: CLAHE, flip, rotation

## 3. Results
### 3.1 Stratified Split Performance
- Full metrics table with CIs
### 3.2 Center-Holdout Performance
- Full metrics table with CIs
### 3.3 Comparison Table
- Side-by-side metrics, generalization gap
### 3.4 Baseline Comparison
- BTXRD paper YOLOv8s-cls results with 7 caveats

## 4. Explainability (Grad-CAM)
- Gallery summary (TP/FP/FN patterns)
- Annotation comparison (mean IoU 0.070)
- Interpretation: model attention vs. expert annotations

## 5. Clinical Relevance
- Clinical decision framing statement (required by success criteria 3)
- Operating point analysis from ROC data
- Sensitivity/specificity tradeoff interpretation

## 6. Limitations
- Same-lesion multi-angle image leakage risk
- Label noise ceiling (radiological, not pathology-confirmed)
- Center generalization gap (0.219 AUC drop)
- Small Malignant test set (51 stratified, 107 center-holdout)
- Wide confidence intervals (27 pp for Malignant sensitivity)
- Low Grad-CAM IoU (0.070)
- Single architecture, no hyperparameter optimization

## 7. Next Steps
- Multi-center validation with patient-level grouping
- Larger Malignant class representation
- Architecture exploration (larger EfficientNet variants, ViT)
- External dataset validation
- Pathology-confirmed label subset
```

### Pattern 3: Clinical Decision Framing Statement

**What:** Translate model metrics into a clinical operating-point statement.
**When to use:** Required by Phase 7 success criteria #3 in the PoC report.

The success criteria states: *"The PoC report includes a clinical decision framing statement (e.g., 'at X% specificity, the model achieves Y% sensitivity for malignant tumors')"*

Using actual data from `results/stratified/metrics_summary.json`:
- Malignant specificity: 95.7%
- Malignant sensitivity: 60.8% (CI: 47.4%-74.3%)

Clinical framing statement:

> "At the default operating point on the stratified test set, the model achieves 60.8% sensitivity (95% CI: 47.4%-74.3%) for malignant tumors at 95.7% specificity. This means approximately 4 in 10 malignant cases would be missed, while fewer than 5 in 100 non-malignant cases would be falsely flagged. The wide confidence interval reflects the small Malignant test sample (n=51) and underscores the need for larger-scale validation before any clinical consideration."

For center-holdout:
- Malignant specificity: 79.9%
- Malignant sensitivity: 36.4% (CI: 27.0%-44.8%)

> "On the center-holdout test set (Centers 2+3, unseen during training), malignant sensitivity drops to 36.4% (95% CI: 27.0%-44.8%) at 79.9% specificity, indicating that nearly two-thirds of malignant cases from novel centers would be missed. This substantial generalization gap demonstrates that the model has not learned center-invariant features and is not ready for deployment outside its training distribution."

### Pattern 4: README Reproduction Guide

**What:** Step-by-step CLI commands for end-to-end reproduction from scratch.
**When to use:** For `README.md` update (success criteria #4).

```markdown
## Quick Start (Full Pipeline)

```bash
# 1. Clone and setup
git clone <repo-url> AssureXRay && cd AssureXRay
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 2. Download and audit data (~801 MB download)
make download    # Downloads BTXRD from figshare -> data_raw/
make audit       # Generates docs/data_audit_report.md

# 3. Create train/val/test splits
make split       # Creates data/splits/*.csv manifests

# 4. Train models (both strategies)
make train-all   # Trains on stratified + center-holdout splits

# 5. Evaluate
make evaluate    # Full metrics, ROC/PR curves, comparison tables

# 6. Explainability
make gradcam     # Grad-CAM galleries and annotation comparison

# 7. Inference on new images
python scripts/infer.py --image path/to/radiograph.jpeg --checkpoint checkpoints/best_stratified.pt
```
```

### Anti-Patterns to Avoid

- **Inventing numbers not in results JSON:** Every metric in the documentation must come directly from a JSON file in `results/`. Do not compute derived statistics by hand -- reference the pre-computed values.
- **Understating limitations:** The project has documented concerns (leakage risk, low Grad-CAM IoU, generalization gap). These MUST appear prominently, not buried in footnotes.
- **Overstating clinical utility:** This is a PoC. The documentation should never suggest the model is ready for clinical use, even in qualified language like "could potentially assist."
- **Inconsistent numbers between documents:** The model card, PoC report, and README must use identical numbers from the same JSON sources. A mismatch between documents undermines credibility.
- **Missing the NOT FOR CLINICAL USE disclaimer:** This must appear in the model card (Intended Use section), the PoC report (Executive Summary and Clinical Relevance), and the README (License and Disclaimer section). It already exists in the current README.
- **Reporting metrics without confidence intervals:** Bare point estimates hide the uncertainty from small test sets. Always pair metrics with bootstrap CIs, especially for Malignant class.
- **Hiding the 7 BTXRD baseline comparison caveats:** The comparison table alone without caveats would be misleading. All 7 caveats must be presented alongside the comparison.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Model card format | Custom documentation structure | Mitchell et al. (2019) 9-section template | Recognized standard in ML community; reviewers and collaborators know where to find information; signals professional documentation practices |
| Metric values | Manual calculation from raw data | Pre-computed JSON files in `results/` | JSON files are the authoritative source; manual calculations risk transcription errors and inconsistency |
| Clinical framing statement | Ad-hoc sensitivity/specificity description | Standard "at X% specificity, Y% sensitivity" format from clinical decision support literature | Familiar format for clinical audiences; immediately interpretable |
| Comparison table | Manually reconstructing from individual metrics | `results/comparison_table.json` and `results/comparison_table.csv` | Already contains stratified, center-holdout, baseline, and gap calculations with proper formatting |

**Key insight:** Phase 7 is about accurate synthesis and honest framing, not computation. Every number already exists. The work is in organizing, contextualizing, and communicating findings clearly.

## Common Pitfalls

### Pitfall 1: Number Inconsistency Across Documents

**What goes wrong:** Model card says macro AUC is 0.846, PoC report says 0.845, README says 0.85.
**Why it happens:** Copy-pasting from different sources or rounding inconsistently.
**How to avoid:** Read numbers directly from the JSON files. Use consistent rounding (3 decimal places for AUC/sensitivity, 1 decimal for percentages, 1 for F1). Cross-reference all three documents before finalizing.
**Warning signs:** Run a text search for key metrics across all three files to verify consistency.

### Pitfall 2: Burying the Generalization Gap

**What goes wrong:** Report leads with stratified results (AUC 0.846) and mentions center-holdout (AUC 0.627) as an afterthought, creating a misleadingly positive impression.
**Why it happens:** Natural tendency to highlight good results.
**How to avoid:** Present both splits with equal prominence. The comparison table should appear early in the results section. The executive summary must mention both. The generalization gap (-0.219 AUC) is a key finding, not a footnote.
**Warning signs:** A reader skimming only the first paragraph would come away thinking the model works well.

### Pitfall 3: Missing Clinical Decision Framing Statement

**What goes wrong:** The PoC report discusses metrics in ML terms (AUC, F1) without translating to clinical impact.
**Why it happens:** ML practitioners default to ML jargon.
**How to avoid:** Success criteria #3 explicitly requires a clinical decision framing statement. Use the "at X% specificity, Y% sensitivity" format. Translate to plain language: "X in Y malignant cases would be missed."
**Warning signs:** No sentence in the report uses the words "sensitivity" and "specificity" together in a clinically interpretable context.

### Pitfall 4: Omitting Confidence Intervals

**What goes wrong:** Report states "Malignant sensitivity is 60.8%" without CIs, implying more precision than the data supports.
**Why it happens:** CIs feel like clutter in tables.
**How to avoid:** Always report CIs for key metrics. The 27-percentage-point width for Malignant sensitivity (0.474-0.743) is itself a finding about the study's statistical power.
**Warning signs:** Any metric reported as a bare number without CI or sample size context.

### Pitfall 5: Incomplete README for Reproduction

**What goes wrong:** A new developer clones the repo, follows README instructions, and gets stuck at some step.
**Why it happens:** README written by someone who knows the project and unconsciously skips steps.
**How to avoid:** Success criteria #4 requires "a new developer can reproduce results from scratch." The README must include: prerequisites, git clone, venv creation, pip install, data download, full pipeline commands, expected outputs at each step.
**Warning signs:** Any step that requires knowledge not in the README (e.g., "you need CUDA" without mentioning the CPU alternative).

### Pitfall 6: Forgetting to Document Leakage Risk

**What goes wrong:** Report presents stratified split results without acknowledging the same-lesion leakage risk.
**Why it happens:** The risk was documented in Phase 2-3 but not carried forward to Phase 7 reports.
**How to avoid:** The leakage risk must appear in: (1) model card Ethical Considerations, (2) PoC report Limitations, (3) alongside stratified results interpretation. This is a known blocker/concern from STATE.md.
**Warning signs:** The word "leakage" does not appear in model card or PoC report.

### Pitfall 7: Not Updating README Project Structure

**What goes wrong:** README shows project structure from Phase 1 that doesn't reflect files added in Phases 2-6.
**Why it happens:** README was written at scaffold time and never updated.
**How to avoid:** The existing README project structure is reasonably complete (it includes results/, docs/, scripts/ etc.) but needs verification. Check that all actual scripts (download.py, audit.py, split.py, train.py, eval.py, gradcam.py, infer.py) and source modules are reflected. Update the Makefile targets section if needed.
**Warning signs:** `ls scripts/` output doesn't match README's project structure section.

## Code Examples

No code is needed for Phase 7. However, here are the data sources that documentation must reference:

### Metrics Source Files

```python
# All numbers in documentation come from these JSON files:

# Stratified split performance
results/stratified/metrics_summary.json
# Key values:
#   macro_auc: 0.846
#   malignant_sensitivity: 0.608
#   accuracy: 0.679
#   per_class_auc: Normal 0.843, Benign 0.788, Malignant 0.906
#   per_class_sensitivity: Normal 0.606, Benign 0.784, Malignant 0.608
#   per_class_specificity: Normal 0.879, Benign 0.625, Malignant 0.957

# Center-holdout split performance
results/center_holdout/metrics_summary.json
# Key values:
#   macro_auc: 0.627
#   malignant_sensitivity: 0.364
#   accuracy: 0.472
#   per_class_auc: Normal 0.653, Benign 0.574, Malignant 0.653
#   per_class_sensitivity: Normal 0.262, Benign 0.643, Malignant 0.364
#   per_class_specificity: Normal 0.895, Benign 0.412, Malignant 0.799

# Bootstrap confidence intervals
results/stratified/bootstrap_ci.json
# Key values:
#   macro_auc: 0.846 (0.814-0.873)
#   malignant_sensitivity: 0.608 (0.474-0.743)

results/center_holdout/bootstrap_ci.json
# Key values:
#   macro_auc: 0.627 (0.594-0.658)
#   malignant_sensitivity: 0.364 (0.270-0.448)

# Comparison table (dual split + baseline + gap)
results/comparison_table.json
# Key values:
#   generalization_gap.macro_auc_gap: -0.219
#   generalization_gap.malignant_sensitivity_gap: -0.243

# Grad-CAM annotation comparison
results/gradcam/annotation_report.json
# Key values:
#   mean_iou: 0.070
#   summary: "1/5 images show focal attention overlapping annotated tumor regions (IoU > 0.1)"

# Classification reports (per-class precision, recall, F1)
results/stratified/classification_report.json
results/center_holdout/classification_report.json
```

### Figure References for Reports

```
# Figures available for embedding in reports:

# Training
results/stratified/loss_curve.png
results/center_holdout/loss_curve.png

# Evaluation (per split)
results/stratified/roc_curves.png
results/stratified/pr_curves.png
results/stratified/confusion_matrix.png
results/stratified/confusion_matrix_normalized.png
results/center_holdout/roc_curves.png
results/center_holdout/pr_curves.png
results/center_holdout/confusion_matrix.png
results/center_holdout/confusion_matrix_normalized.png

# Grad-CAM
results/gradcam/gallery_Normal.png
results/gradcam/gallery_Benign.png
results/gradcam/gallery_Malignant.png
results/gradcam/annotation_comparison.png

# Data audit (already in docs)
docs/figures/class_distribution.png
docs/figures/dimension_histogram.png
docs/figures/center_breakdown.png
docs/figures/annotation_coverage.png
docs/figures/duplicate_detection.png
```

### Existing README Content to Preserve/Extend

The current README (104 lines) already has:
- Project title and overview (good, keep as-is)
- Setup instructions (basic, needs expansion)
- Project structure (accurate, verify completeness)
- Available commands table (accurate, keep)
- Configuration section (good, keep)
- License and Disclaimer with NOT FOR CLINICAL USE (critical, keep)

Updates needed:
- Add "Data Download" section with specific commands and expected output
- Add "Training" section with both split strategies
- Add "Evaluation" section with expected output files
- Add "Inference" section with concrete examples
- Add "Key Results" section with summary metrics table
- Verify project structure matches actual file tree
- Add "Reproduction" section with full pipeline commands

## Data Inventory for Documentation

### Concrete Numbers to Include

All values below are from the authoritative JSON sources:

**Stratified Split (test set n=564):**

| Metric | Value | 95% CI |
|--------|-------|--------|
| Macro AUC | 0.846 | 0.814-0.873 |
| Accuracy | 67.9% | - |
| Normal Sensitivity | 60.6% | 0.546-0.663 |
| Benign Sensitivity | 78.4% | 0.728-0.836 |
| Malignant Sensitivity | 60.8% | 0.474-0.743 |
| Normal Specificity | 87.9% | - |
| Benign Specificity | 62.5% | - |
| Malignant Specificity | 95.7% | - |
| Malignant AUC | 0.906 | - |

**Center-Holdout Split (test set n=808):**

| Metric | Value | 95% CI |
|--------|-------|--------|
| Macro AUC | 0.627 | 0.594-0.658 |
| Accuracy | 47.2% | - |
| Normal Sensitivity | 26.2% | 0.209-0.320 |
| Benign Sensitivity | 64.3% | 0.599-0.688 |
| Malignant Sensitivity | 36.4% | 0.270-0.448 |
| Normal Specificity | 89.5% | - |
| Benign Specificity | 41.2% | - |
| Malignant Specificity | 79.9% | - |

**Generalization Gap (center-holdout minus stratified):**

| Metric | Gap |
|--------|-----|
| Macro AUC | -0.219 |
| Malignant Sensitivity | -0.243 |
| Accuracy | -0.208 |

**Training Details:**
- Stratified: 5 epochs (early stopping), best at epoch 5
- Center-holdout: 5 epochs (early stopping), best at epoch 5
- Optimizer: Adam, lr=0.001, weight_decay=0.0001
- Scheduler: Cosine annealing
- Batch size: 32
- Early stopping patience: 7

**Model Architecture:**
- Backbone: EfficientNet-B0 (timm 1.0.15)
- Pretrained: ImageNet
- Parameters: 4,011,391 total (all trainable, full fine-tuning)
- Input: 224x224x3, ImageNet normalized
- Output: 3 logits (no softmax, use CrossEntropyLoss)
- Dropout: 0.2

**Dataset:**
- Total images: 3,746
- Normal: 1,879 (50.2%), Benign: 1,525 (40.7%), Malignant: 342 (9.1%)
- Centers: Center 1 (78.4%), Center 2 (14.7%), Center 3 (6.9%)
- License: CC BY-NC-ND 4.0

**Grad-CAM:**
- Mean IoU against annotations: 0.070
- Only 1/5 analyzed images had IoU > 0.1
- Target layer: model.bn2 (BatchNormAct2d, 1280 channels)
- Binarization threshold: 0.5

### Limitations Checklist

These MUST all appear in the PoC report limitations section and most in the model card:

1. **Same-lesion multi-angle image leakage risk** -- No patient_id available; same lesion from different angles may appear in both train and test for stratified split. Center-holdout partially mitigates.
2. **Label noise ceiling** -- Some diagnoses are radiologist-empirical, not pathology-confirmed. The model cannot exceed the accuracy of its labels.
3. **Center generalization gap** -- Macro AUC drops from 0.846 to 0.627 (-0.219) when testing on unseen centers, indicating the model learned center-specific rather than disease-specific features.
4. **Absence of pathology-confirmed labels** -- Ground truth is radiological assessment, which is imperfect.
5. **Small Malignant test set** -- Only 51 Malignant samples in stratified test, 107 in center-holdout. This drives the wide CI (27 percentage points).
6. **Low Grad-CAM IoU (0.070)** -- Model attention weakly correlated with expert tumor annotations. Model may rely on contextual features (bone shape, surrounding tissue) rather than tumor region directly.
7. **Single architecture** -- Only EfficientNet-B0 evaluated. No architecture comparison, no ensemble, no hyperparameter optimization.
8. **Center 3 Normal class sparsity** -- Only 27 Normal images from Center 3, making per-center evaluation unreliable for this subgroup.
9. **BTXRD baseline comparison caveats** -- 7 documented caveats make direct comparison with YOLOv8s-cls baseline inappropriate for performance ranking (different split, metric, architecture, image size, training duration, no separate test set, no macro AUC reported).

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Ad-hoc model documentation | Mitchell et al. (2019) model cards | 2019 | Standardized structure for transparent reporting; adopted by HuggingFace, Google, and major ML venues |
| ML-only metric reporting | Clinical framing (sensitivity/specificity at operating points) | 2020+ | Bridges gap between ML researchers and clinicians; required for clinical AI papers |
| Single-split reporting | Multi-split with generalization analysis | 2020+ | Standard practice in medical imaging to test on out-of-distribution data |
| Point estimates only | Bootstrap confidence intervals | Long-standing but now expected | Essential for small test sets; reviewers expect CIs in medical AI papers |
| README as afterthought | README as primary onboarding document | 2020+ | GitHub-centric workflows make README the first thing collaborators see |

**Deprecated/outdated:**
- **Single-page model summaries:** Model cards replaced ad-hoc model descriptions with structured, comparable formats
- **Reporting only accuracy:** Medical imaging now expects per-class sensitivity/specificity, AUC with CIs, and clinical operating point analysis

## Open Questions

1. **Figure embedding in Markdown**
   - What we know: The PoC report should reference figures from `results/` (ROC curves, confusion matrices, Grad-CAM galleries). Markdown supports relative image paths (`![Caption](../results/stratified/roc_curves.png)`).
   - What's unclear: Whether to use relative paths from `docs/` (e.g., `../results/`) or absolute paths from repo root (e.g., `results/`). GitHub rendering handles both but relative paths are more portable.
   - Recommendation: Use relative paths from the file's location. For `docs/poc_report.md`, use `../results/stratified/roc_curves.png`. For `README.md`, use `results/stratified/roc_curves.png`.

2. **Report length and detail level**
   - What we know: Success criteria specify minimum content (executive summary, methods, results, limitations, etc.) but not maximum length.
   - What's unclear: How much detail to include in methods (e.g., full augmentation pipeline description vs. summary).
   - Recommendation: Aim for comprehensive but scannable. Use tables for metrics, prose for interpretation, bullet points for lists. Model card: ~200-300 lines. PoC report: ~400-600 lines. README: ~150-200 lines.

3. **Makefile report target**
   - What we know: The Makefile has a `report` target that currently just prints "Report generation is manual -- see docs/". Phase 7 produces manual documents, not generated ones.
   - What's unclear: Whether to update the `report` target to do something useful (e.g., validate that docs exist, print their locations).
   - Recommendation: Leave the `report` target as-is or update to a simple check: `@ls docs/model_card.md docs/poc_report.md 2>/dev/null && echo "Reports available in docs/" || echo "Run Phase 7 to generate reports"`. This is a very minor detail and can be handled in plan 07-02.

## Sources

### Primary (HIGH confidence)
- Mitchell et al., "Model Cards for Model Reporting," FAccT 2019 (https://arxiv.org/abs/1810.03993) -- Defines the 9-section model card format
- vetiver model card documentation (https://rstudio.github.io/vetiver-r/articles/model-card.html) -- Practical template with section descriptions
- FAU model card template (https://github.com/fau-masters-collected-works-cgarbin/model-card-template) -- GitHub Markdown template
- Project `results/` directory JSON files -- Authoritative source for all metrics
- Project `docs/` existing documents -- Data audit report and dataset spec (Phase 2)
- Project STATE.md, REQUIREMENTS.md, ROADMAP.md -- Success criteria and accumulated decisions

### Secondary (MEDIUM confidence)
- NHS England model card template (https://github.com/nhsengland/model-card) -- Healthcare-specific model card template
- CHAI (Coalition for Health AI) model card framework -- Emerging standard for healthcare AI documentation (https://pmc.ncbi.nlm.nih.gov/articles/PMC11861263/)
- HuggingFace model card guidelines (https://huggingface.co/docs/hub/model-cards) -- Community standard for model documentation
- Clinical decision support sensitivity/specificity framing (https://pmc.ncbi.nlm.nih.gov/articles/PMC11073764/)

### Tertiary (LOW confidence)
- None -- all findings verified with primary or secondary sources

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- No libraries needed; documentation-only phase with well-established templates
- Architecture: HIGH -- Mitchell et al. (2019) format is an established standard; all data sources verified and complete
- Pitfalls: HIGH -- Based on concrete project data and documented concerns from prior phases
- Clinical framing: MEDIUM -- Framing statement format is standard but specific wording needs clinical domain review

**Research date:** 2026-02-20
**Valid until:** 2026-06-20 (documentation standards are stable; project data is fixed)
