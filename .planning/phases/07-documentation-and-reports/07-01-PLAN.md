---
phase: 07-documentation-and-reports
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/model_card.md
  - docs/poc_report.md
autonomous: true

must_haves:
  truths:
    - "Model card follows Mitchell et al. (2019) format with all required sections"
    - "Model card contains NOT FOR CLINICAL USE disclaimer prominently"
    - "PoC report contains executive summary, methods, results, limitations, and next steps"
    - "PoC report includes clinical decision framing statement with sensitivity at specificity"
    - "All metrics match results JSON files exactly (no invented numbers)"
    - "All 9 documented limitations appear in the report"
    - "Both splits are compared side-by-side with bootstrap CIs"
  artifacts:
    - path: "docs/model_card.md"
      provides: "Mitchell et al. 2019 format model card"
      min_lines: 150
    - path: "docs/poc_report.md"
      provides: "Comprehensive PoC report with clinical framing"
      min_lines: 300
  key_links:
    - from: "docs/model_card.md"
      to: "results/stratified/metrics_summary.json"
      via: "Performance metrics must match"
      pattern: "0\\.846|0\\.608|0\\.627"
    - from: "docs/poc_report.md"
      to: "results/comparison_table.json"
      via: "Dual split comparison table data"
      pattern: "generalization.*gap|center.holdout"
---

<objective>
Create the model card and PoC report -- the two primary documentation deliverables for Phase 7.

Purpose: These documents synthesize all project findings into auditable, clinician-inspectable artifacts. The model card provides a standardized technical reference. The PoC report provides clinical context, results interpretation, and honest limitation analysis.

Output: `docs/model_card.md` and `docs/poc_report.md`
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

## Data Sources (read these JSON files for exact numbers -- NEVER invent metrics)

@results/stratified/metrics_summary.json
@results/center_holdout/metrics_summary.json
@results/stratified/bootstrap_ci.json
@results/center_holdout/bootstrap_ci.json
@results/comparison_table.json
@results/stratified/classification_report.json
@results/center_holdout/classification_report.json
@results/gradcam/annotation_report.json
@configs/default.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create model card (docs/model_card.md)</name>
  <files>docs/model_card.md</files>
  <action>
Create `docs/model_card.md` following the Mitchell et al. (2019) "Model Cards for Model Reporting" format.

**Required sections (in order):**

1. **Header**: Title "AssureXRay -- Bone Tumor Classification Model Card", date, version "v1.0-PoC"

2. **Model Details**:
   - Organization: Research proof-of-concept
   - Architecture: EfficientNet-B0 (timm 1.0.15), 4,011,391 parameters
   - Full fine-tuning from ImageNet pretrained weights
   - Input: 224x224 RGB radiograph images
   - Output: 3-class softmax (Normal, Benign, Malignant)
   - Training: Adam optimizer, lr=0.001, cosine annealing, batch 32, early stopping patience 7, weighted cross-entropy
   - Framework: PyTorch 2.6.0 / torchvision 0.21.0

3. **Intended Use**:
   - Primary: Research proof-of-concept for bone tumor radiograph classification feasibility
   - Out of scope: Clinical diagnosis, treatment planning, screening
   - Users: Researchers, ML engineers evaluating bone tumor classification approaches
   - **NOT FOR CLINICAL USE** (bold, prominent)

4. **Factors**:
   - Relevant factors: imaging center (source bias -- 78% from Center 1), anatomical site (14 sites in dataset), tumor subtype
   - Demographic factors: age, gender (available in dataset but not analyzed for subgroup fairness in this PoC)
   - Environmental: radiograph quality varies by center, imaging equipment differences

5. **Metrics**:
   - Primary metric: Macro AUC (measures discrimination ability across all 3 classes)
   - Headline clinical metric: Malignant sensitivity (missing cancer is worse than false alarm in screening context)
   - Supporting: per-class AUC, sensitivity, specificity, precision, recall, F1
   - Uncertainty: Bootstrap 95% CIs (1000 iterations, percentile method)
   - Why these metrics: AUC is threshold-independent; sensitivity/specificity enable clinical operating point analysis

6. **Training Data**:
   - BTXRD dataset: 3,746 radiographs from 3 centers
   - Class distribution: Normal 1,879 (50.2%), Benign 1,525 (40.7%), Malignant 342 (9.1%)
   - License: CC BY-NC-ND 4.0
   - Citation: Yao et al., Scientific Data 2025

7. **Evaluation Data**:
   - Stratified split: 564 test images (282 Normal, 231 Benign, 51 Malignant)
   - Center-holdout split: 808 test images (286 Normal, 415 Benign, 107 Malignant)

8. **Quantitative Analyses** -- TWO tables (stratified and center-holdout):

   **Stratified split table:**
   | Metric | Value | 95% CI |
   |--------|-------|--------|
   | Macro AUC | 0.846 | 0.814-0.873 |
   | Accuracy | 67.9% | -- |
   | Malignant Sensitivity | 60.8% | 47.4%-74.3% |
   | Malignant Specificity | 95.7% | -- |
   | Benign Sensitivity | 78.4% | 72.8%-83.6% |
   | Normal Sensitivity | 60.6% | 54.6%-66.3% |

   **Center-holdout split table:**
   | Metric | Value | 95% CI |
   |--------|-------|--------|
   | Macro AUC | 0.627 | 0.594-0.658 |
   | Accuracy | 47.2% | -- |
   | Malignant Sensitivity | 36.4% | 27.0%-44.8% |
   | Malignant Specificity | 79.9% | -- |

   **Generalization gap table:**
   | Metric | Gap |
   |--------|-----|
   | Macro AUC | -0.219 |
   | Malignant Sensitivity | -0.243 |
   | Accuracy | -0.208 |

9. **Caveats and Recommendations** (all 9 limitations + recommendations):
   1. Same-lesion multi-angle image leakage risk (no patient_id available)
   2. Label noise ceiling (radiologist diagnosis, not pathology-confirmed)
   3. Substantial center generalization gap (-0.219 AUC)
   4. Absence of pathology-confirmed labels
   5. Small Malignant test set (51 stratified, 107 center-holdout)
   6. Wide confidence intervals (27 pp for Malignant sensitivity)
   7. Low Grad-CAM IoU with expert annotations (mean 0.070)
   8. Single architecture evaluated (no comparison, ensemble, or HPO)
   9. Center 3 Normal class sparsity (27 images)

10. **Ethical Considerations**:
   - Non-clinical use only disclaimer
   - Dataset bias: 78% from Chinese hospitals (Center 1)
   - Potential for misuse if deployed as clinical tool
   - CC BY-NC-ND 4.0 restricts commercial use

   **Recommendations** (include within Caveats and Recommendations above):
   - Do not use for clinical decision-making
   - Patient-level split with pathology-confirmed labels needed
   - Multi-center external validation required before any clinical consideration

**Critical rules:**
- All numbers MUST come from the JSON result files -- never round differently than what's in the files
- Round to 3 decimal places for AUC, 1 decimal for percentages
- Always pair point estimates with bootstrap CIs where available
- Use `0.846` format for AUC, `60.8%` format for percentages
  </action>
  <verify>
Verify file exists and contains all required sections:
```bash
test -f docs/model_card.md && \
grep -c "Model Details\|Intended Use\|Factors\|Metrics\|Training Data\|Evaluation Data\|Quantitative\|Caveats\|Ethical" docs/model_card.md
```
Should return 9 (all Mitchell et al. 2019 sections present).

Also verify key numbers:
```bash
grep "0.846" docs/model_card.md && grep "0.627" docs/model_card.md && grep "NOT FOR CLINICAL USE" docs/model_card.md
```
  </verify>
  <done>
`docs/model_card.md` exists with all 9 Mitchell et al. sections, both split performance tables with CIs, all 9 limitations documented, and NOT FOR CLINICAL USE disclaimer present.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create PoC report (docs/poc_report.md)</name>
  <files>docs/poc_report.md</files>
  <action>
Create `docs/poc_report.md` as a comprehensive proof-of-concept report.

**Required sections (in order):**

1. **Title and Metadata**: "AssureXRay: Proof-of-Concept Report -- 3-Class Bone Tumor Classification from Radiographs", date, version v1.0

2. **Disclaimer**: Prominent box/section: "NOT FOR CLINICAL USE -- This is a research proof-of-concept. The model has not been validated for diagnostic purposes."

3. **Executive Summary** (~200 words):
   - What: 3-class bone tumor classifier (Normal/Benign/Malignant) on BTXRD dataset
   - How: EfficientNet-B0 with weighted CE, dual split evaluation
   - Key finding: Stratified macro AUC 0.846, but center-holdout AUC drops to 0.627
   - Conclusion: Feasibility demonstrated for within-distribution data; cross-center generalization requires further work
   - Include the clinical decision framing statement: "At the default operating point, the model achieves 60.8% sensitivity (95% CI: 47.4%-74.3%) for malignant tumors at 95.7% specificity on the stratified test set. On the center-holdout test set, malignant sensitivity drops to 36.4% (95% CI: 27.0%-44.8%) at 79.9% specificity."

4. **Methods**:
   - **4.1 Dataset**: BTXRD description, 3,746 images, 3 centers, class distribution, CC BY-NC-ND 4.0. Reference Yao et al. 2025.
   - **4.2 Data Processing**: CLAHE augmentation, ImageNet normalization, 224x224 resize, horizontal flip + rotation for training
   - **4.3 Split Strategy**: Explain BOTH strategies:
     - Stratified: image-level 70/15/15 with duplicate-aware grouping
     - Center-holdout: Center 1 train/val, Centers 2+3 test
     - Why both: stratified for standard performance; center-holdout for generalization assessment
   - **4.4 Model Architecture**: EfficientNet-B0, 4M params, full fine-tuning, ImageNet init
   - **4.5 Training**: Adam lr=0.001, cosine annealing, batch 32, weighted CE (inverse class frequency), early stopping patience 7
   - **4.6 Evaluation**: Metrics enumeration (macro AUC, per-class AUC, sensitivity, specificity, F1, confusion matrix), bootstrap CIs (1000 iterations, percentile method)

5. **Results**:
   - **5.1 Stratified Split**: Full metrics table with CIs from `results/stratified/metrics_summary.json` and `results/stratified/bootstrap_ci.json`. Include per-class breakdown. Reference confusion matrix figure.
   - **5.2 Center-Holdout Split**: Same format from center_holdout results. Highlight the performance degradation.
   - **5.3 Comparison Table**: Side-by-side comparison matching `results/comparison_table.json`. Show generalization gap explicitly.
   - **5.4 BTXRD Baseline Comparison**: Compare against YOLOv8s-cls results from the paper. Read the `caveats` array from `results/comparison_table.json` (7 entries) and list all 7 verbatim or paraphrased -- do not summarize or omit any.

6. **Explainability (Grad-CAM)**:
   - Method: Grad-CAM targeting predicted class, layer model.bn2 (BatchNormAct2d 1280), threshold 0.5
   - Qualitative findings: Gallery images (reference results/gradcam/ figures)
   - Annotation comparison: Mean IoU 0.070, interpretation (weak alignment between model attention and expert-annotated tumor regions)
   - What this means: Model may be using contextual cues rather than focal tumor features

7. **Limitations** (all 9, expanded with context):
   1. **Leakage risk**: Same-lesion multi-angle images may appear in both train and test due to no patient_id. Proxy grouping (center+age+gender+site) yielded 941 groups but does not guarantee patient isolation.
   2. **Label noise**: Diagnoses are radiologist opinions, not pathology-confirmed. This creates an inherent performance ceiling.
   3. **Center generalization gap**: -0.219 AUC drop from stratified to center-holdout. Model overfits to Center 1 imaging characteristics.
   4. **No pathology-confirmed labels**: Same as #2 but from clinical validity angle.
   5. **Small Malignant cohort**: Only 51 stratified / 107 center-holdout test Malignant samples.
   6. **Wide CIs**: Malignant sensitivity CI spans 27 percentage points (47.4%-74.3% stratified).
   7. **Low Grad-CAM IoU (0.070)**: Model attention does not strongly overlap expert tumor annotations.
   8. **Single architecture**: No architecture comparison, no ensemble, no HPO. Cannot claim optimality.
   9. **Center 3 sparsity**: Only 27 Normal images in center-holdout test.

8. **Clinical Relevance**:
   - Frame as feasibility demonstration, NOT clinical tool
   - Include clinical decision framing statement (same as executive summary)
   - Discuss what would be needed for clinical utility: pathology-confirmed labels, patient-level splits, external validation, regulatory pathway
   - Explicitly state the model should NOT be used for clinical decisions in its current form

9. **Recommended Next Steps** (5-7 bullets):
   - Patient-level splitting (requires patient_id or reliable proxy)
   - Pathology-confirmed label subset for validation
   - Multi-architecture comparison (ResNet, ConvNeXt, ViT)
   - External dataset validation
   - Hyperparameter optimization
   - Larger Malignant cohort acquisition
   - Prospective clinical study design (if results warrant)

10. **References**:
    - Yao et al., Scientific Data 2025
    - Mitchell et al., Model Cards for Model Reporting, FAT* 2019
    - Selvaraju et al., Grad-CAM, ICCV 2017
    - Tan & Le, EfficientNet, ICML 2019

11. **Appendix**:
    - Figure references (loss curves, ROC curves, PR curves, confusion matrices, Grad-CAM galleries, annotation comparison)
    - Full per-class metrics tables

**Critical rules:**
- All numbers MUST come from JSON result files
- Clinical decision framing statement MUST appear in both executive summary and clinical relevance section
- All 7 BTXRD baseline comparison caveats MUST be included
- Do NOT overstate clinical utility -- this is a PoC
- Do NOT understate limitations
- Reference figure paths as relative (e.g., `../results/stratified/roc_curves.png`)
  </action>
  <verify>
Verify file exists and contains all required sections:
```bash
test -f docs/poc_report.md && \
grep -c "Executive Summary\|Methods\|Results\|Explainability\|Limitations\|Clinical Relevance\|Next Steps\|References" docs/poc_report.md
```
Should return 8+ (all major sections present).

Verify clinical decision framing statement appears at least twice:
```bash
grep -c "60.8% sensitivity" docs/poc_report.md
```
Should return >= 2.

Verify NOT FOR CLINICAL USE:
```bash
grep "NOT FOR CLINICAL USE" docs/poc_report.md
```

Verify all 7 BTXRD caveats present:
```bash
grep -c "caveat\|Caveat\|different\|Random 80/20\|mAP" docs/poc_report.md
```
  </verify>
  <done>
`docs/poc_report.md` exists with all 11 sections, clinical decision framing statement in executive summary and clinical relevance, NOT FOR CLINICAL USE disclaimer, all 9 limitations expanded, all 7 BTXRD baseline caveats, and figure references to results/ directory.
  </done>
</task>

</tasks>

<verification>
1. Both files exist and are non-empty:
   ```bash
   wc -l docs/model_card.md docs/poc_report.md
   ```
   model_card.md should be 150+ lines, poc_report.md should be 300+ lines.

2. Numbers consistency check -- these numbers must appear in both documents:
   - Stratified macro AUC: 0.846 (or 0.8456)
   - Center-holdout macro AUC: 0.627 (or 0.6267)
   - Malignant sensitivity stratified: 60.8% (or 0.608)
   - Malignant sensitivity center-holdout: 36.4% (or 0.364)
   - Generalization gap: -0.219

3. NOT FOR CLINICAL USE appears in both files.

4. All 9 limitations documented in both files (model card lists them; PoC report expands them).
</verification>

<success_criteria>
- `docs/model_card.md` follows Mitchell et al. (2019) format with all 9 sections
- `docs/poc_report.md` contains executive summary, methods, results, Grad-CAM findings, limitations, clinical relevance, and next steps
- Clinical decision framing statement present in PoC report (executive summary + clinical relevance)
- All metrics sourced from JSON result files with no invented numbers
- NOT FOR CLINICAL USE disclaimer in both documents
- All 9 limitations documented
- Both split strategies compared with generalization gap quantified
</success_criteria>

<output>
After completion, create `.planning/phases/07-documentation-and-reports/07-01-SUMMARY.md`
</output>
