---
phase: 02-data-acquisition-and-audit
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - scripts/audit.py
  - docs/data_audit_report.md
  - docs/figures/class_distribution.png
  - docs/figures/dimension_histogram.png
  - docs/figures/center_breakdown.png
  - docs/figures/missing_values.png
  - docs/figures/annotation_coverage.png
  - docs/figures/duplicate_detection.png
  - docs/dataset_spec.md
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/audit.py --config configs/default.yaml` generates docs/data_audit_report.md with embedded figures"
    - "The audit report confirms class distribution: ~1,879 Normal / ~1,525 Benign / ~342 Malignant"
    - "The audit report includes image dimension histogram, per-center breakdown, missing value counts, annotation coverage, and duplicate detection"
    - "The audit report explicitly documents the leakage risk from same-lesion multi-angle images (no patient_id)"
    - "The audit report notes which metadata columns are available for proxy patient grouping"
    - "docs/dataset_spec.md documents all 37 columns using actual CSV header names, label derivation logic, data provenance per center, and CC BY-NC-ND 4.0 license"
  artifacts:
    - path: "scripts/audit.py"
      provides: "Complete data audit pipeline generating report and figures"
      contains: "audit_class_distribution"
    - path: "docs/data_audit_report.md"
      provides: "Auto-generated audit report with all required sections"
      contains: "Leakage Risk"
    - path: "docs/dataset_spec.md"
      provides: "Dataset specification documenting all 37 columns"
      contains: "CC BY-NC-ND 4.0"
    - path: "docs/figures/class_distribution.png"
      provides: "Bar chart showing Normal/Benign/Malignant counts"
    - path: "docs/figures/dimension_histogram.png"
      provides: "Width and height distribution histograms"
    - path: "docs/figures/center_breakdown.png"
      provides: "Stacked bar chart of class distribution per center"
  key_links:
    - from: "scripts/audit.py"
      to: "data_raw/dataset.csv"
      via: "pd.read_csv for class distribution, missing values, center breakdown"
      pattern: "pd\\.read_csv"
    - from: "scripts/audit.py"
      to: "data_raw/images/"
      via: "PIL.Image.open for dimension measurement, imagehash for duplicate detection"
      pattern: "Image\\.open"
    - from: "scripts/audit.py"
      to: "docs/data_audit_report.md"
      via: "write_audit_report assembles sections with figure references"
      pattern: "figures/"
    - from: "docs/data_audit_report.md"
      to: "docs/figures/*.png"
      via: "Markdown image references with relative paths"
      pattern: "!\\[.*\\]\\(figures/"
---

<objective>
Implement the audit script that profiles the BTXRD dataset and generates `docs/data_audit_report.md` with embedded figures. Also create `docs/dataset_spec.md` documenting all 37 columns, label derivation, provenance, and license.

Purpose: The audit surfaces data quality issues, class imbalance, and leakage risks BEFORE any modeling begins. The dataset spec provides a reference document for all downstream phases. These are the foundation of trust in the data pipeline.

Output: Working `scripts/audit.py`, auto-generated `docs/data_audit_report.md` with 6 embedded figures in `docs/figures/`, and `docs/dataset_spec.md`.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-acquisition-and-audit/02-RESEARCH.md
@.planning/phases/02-data-acquisition-and-audit/02-01-SUMMARY.md
@configs/default.yaml
@scripts/audit.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement the audit script with all required sections</name>
  <files>
    scripts/audit.py
  </files>
  <action>
    Replace the placeholder NotImplementedError in scripts/audit.py with a complete audit implementation. Keep the existing structure (shebang, docstring, PROJECT_ROOT, argparse, load_config, set_seed).

    The script must use `matplotlib.use("Agg")` BEFORE importing pyplot (headless rendering -- no GUI needed).

    **New imports needed** (add after existing imports):
    - `import json`
    - `import matplotlib; matplotlib.use("Agg")`
    - `import matplotlib.pyplot as plt`
    - `import seaborn as sns`
    - `import pandas as pd`
    - `import numpy as np`
    - `import imagehash`
    - `from PIL import Image`
    - `from collections import defaultdict`
    - `from src.utils.logging import setup_logging`

    **Helper function: `save_figure(fig, name, figures_dir)`**
    - Save figure as PNG at `figures_dir / f"{name}.png"` with dpi=150, bbox_inches="tight", facecolor="white"
    - Close the figure after saving (plt.close(fig))
    - Return the relative path string `f"figures/{name}.png"` (relative to docs/ where the report lives)

    **Helper function: `glob_images(images_dir: Path) -> list[Path]`**
    - Return sorted list of JPEG files using case-insensitive patterns: `*.[jJ][pP][gG]` and `*.[jJ][pP][eE][gG]`
    - Combine both glob results and sort

    **Helper function: `derive_label(row) -> str`**
    - If row["malignant"] == 1: return "Malignant"
    - Elif row["benign"] == 1: return "Benign"
    - Else: return "Normal"
    - Used to add a "label" column to the dataframe

    **Section functions (each returns a tuple of (figure_or_None, markdown_text)):**

    1. `audit_class_distribution(df: pd.DataFrame, figures_dir: Path) -> tuple[str, str]`
       - Compute value_counts on the "label" column (order: Normal, Benign, Malignant)
       - Create bar chart with colors: Normal=#2ecc71 (green), Benign=#3498db (blue), Malignant=#e74c3c (red)
       - Add count labels above each bar
       - Title: "Class Distribution"
       - Generate markdown table with Class, Count, Percentage columns
       - Return (figure_path, markdown_text)

    2. `audit_image_dimensions(images_dir: Path, figures_dir: Path) -> tuple[str, str]`
       - Read width and height from each JPEG using `Image.open(path).size` (reads header only, not full pixels)
       - Use tqdm for progress since this iterates all ~3,746 images
       - Create a 1x2 subplot: left=width histogram (50 bins), right=height histogram (50 bins)
       - Title: "Image Dimension Distribution"
       - Generate markdown with min/max/mean/median for both width and height
       - Return (figure_path, summary_text)

    3. `audit_per_center(df: pd.DataFrame, figures_dir: Path) -> tuple[str, str]`
       - Use `pd.crosstab(df["center"], df["label"])` to get per-center class counts
       - Reindex columns to ["Normal", "Benign", "Malignant"]
       - Create stacked bar chart with same color scheme
       - Generate markdown table with center, Normal, Benign, Malignant, Total columns
       - Include percentage of total dataset for each center
       - Return (figure_path, summary_text)

    4. `audit_missing_values(df: pd.DataFrame, figures_dir: Path) -> tuple[str | None, str]`
       - Compute `df.isna().sum()` for each column
       - If any missing values exist: create horizontal bar chart of columns with missing values
       - If no missing values: skip the figure (return None for figure path)
       - Generate markdown table: Column, Missing Count, Percentage
       - Return (figure_path_or_None, summary_text)

    5. `audit_annotation_coverage(df: pd.DataFrame, images_dir: Path, annot_dir: Path, figures_dir: Path) -> tuple[str, str]`
       - Get set of tumor image_ids from df where tumor==1
       - Get set of annotation file stems from annot_dir (*.json glob, extract stem)
       - IMPORTANT: The image_id in dataset.csv may or may not include the file extension. Check if image_ids match annotation stems directly, or if they match after stripping extensions. Handle both cases.
       - Compute: covered (intersection), missing_annotations (tumor - annotations), extra_annotations (annotations - tumor)
       - Create a simple 2-bar chart or pie chart showing "With Annotation" vs "Without Annotation" for tumor images
       - Generate markdown with counts and percentages
       - Note that normal images (tumor=0) are NOT expected to have annotations
       - Return (figure_path, summary_text)

    6. `audit_duplicate_detection(images_dir: Path, figures_dir: Path) -> tuple[str, str]`
       - Compute perceptual hash (phash) for each image using `imagehash.phash(Image.open(path), hash_size=8)`
       - Use tqdm for progress
       - Find exact duplicates: images with identical phash (distance = 0)
       - Find near-duplicates: images with phash Hamming distance <= 5
       - For the near-duplicate search, use an efficient approach:
         - Group images by hash value (dict of hash -> list of filenames) for exact duplicates
         - For near-duplicates with distance 1-5: iterate pairs only among images with close hashes (or if image count is manageable at ~3,746, do the O(n^2) comparison -- it's ~7M integer subtractions, fast enough)
       - Create a histogram of pairwise distances (sample if >10K pairs) or a bar chart showing count of exact vs near-duplicate pairs
       - Generate markdown listing exact duplicate groups and near-duplicate pairs (show top 20 if many)
       - Return (figure_path, summary_text)

    7. `audit_leakage_risk(df: pd.DataFrame) -> str`
       - This is text-only, no figure needed
       - Document the same-lesion multi-angle image risk: no patient_id column exists
       - List the shooting angle distribution from df (frontal, lateral, oblique counts)
       - Create a table of proxy grouping column combinations and their estimated uniqueness:
         - center + age + gender: count unique combinations
         - center + age + gender + anatomical_site: count unique combinations (use the anatomical site columns to derive site -- whichever site column equals 1)
       - Explicitly state the recommendation: acknowledge limitation honestly, center-holdout split partially mitigates
       - Return markdown_text

    **Main report assembly function: `write_audit_report(sections: list[tuple[str, str, str]], output_path: Path)`**
    - Takes a list of (section_title, figure_relative_path_or_None, markdown_text) tuples
    - Writes the complete markdown report:
      - Title: "# BTXRD Data Audit Report"
      - Timestamp: "*Auto-generated by `scripts/audit.py` on {date}*"
      - Table of contents (linked section headers)
      - For each section: ## heading, figure embed if not None (`![{title}]({figure_path})`), then markdown text
    - Write to output_path

    **main() function flow:**
    1. Parse args, load config, set seed, setup logging
    2. Resolve paths: raw_dir, images_dir (raw_dir / "images"), annot_dir (try "Annotations" then "annotations"), csv_path (raw_dir / "dataset.csv"), docs_dir (PROJECT_ROOT / cfg["paths"]["docs_dir"]), figures_dir (docs_dir / "figures")
    3. Validate that raw_dir, images_dir, csv_path exist -- raise FileNotFoundError with helpful message ("Run `make download` first") if missing
    4. Load CSV: `df = pd.read_csv(csv_path)`
    5. Print actual column names: `logger.info(f"CSV columns ({len(df.columns)}): {df.columns.tolist()}")`
    6. Add "label" column using derive_label
    7. Run each audit section function, collecting results
    8. Assemble and write the report to docs/data_audit_report.md
    9. Print summary: "Audit report written to {output_path} with {N} figures"

    **Important implementation details:**
    - Use `matplotlib.use("Agg")` before any pyplot import for headless operation
    - Set seaborn style at start: `sns.set_theme(style="whitegrid", palette="muted")`
    - All figure text should be readable (fontsize >= 10)
    - The report must be valid GitHub-flavored markdown
    - Handle edge cases: if annotations directory doesn't exist (log warning, skip annotation coverage section)
    - For the anatomical site derivation in leakage risk section: iterate over the 15 anatomical site columns, find which one(s) equal 1 for each row. The exact column names come from the actual CSV (read them from df.columns, filtering by known anatomical site names from research). If column names use hyphens or spaces, handle accordingly.
  </action>
  <verify>
    Syntax check: `python -c "import ast; ast.parse(open('scripts/audit.py').read()); print('audit.py: valid syntax')"` passes.

    Help check: `python scripts/audit.py --help` prints usage without errors.

    Full run: `python scripts/audit.py --config configs/default.yaml` completes without errors and prints the column names and summary.

    Report exists: `test -f docs/data_audit_report.md && echo "Report exists"` passes.

    Figures exist: `ls docs/figures/*.png | wc -l` shows at least 5 PNG files.

    Report has required sections: `grep -c "##" docs/data_audit_report.md` shows at least 7 section headers.

    Class distribution confirmed: `grep -E "1,?879|1879" docs/data_audit_report.md` finds the Normal count.

    Leakage risk documented: `grep -i "leakage\|patient_id\|multi-angle" docs/data_audit_report.md` finds leakage discussion.

    Proxy grouping documented: `grep -i "proxy\|grouping" docs/data_audit_report.md` finds proxy grouping discussion.
  </verify>
  <done>
    scripts/audit.py generates docs/data_audit_report.md with embedded figures showing class distribution (confirming ~1,879/1,525/342), image dimension histogram, per-center breakdown, missing value counts, annotation coverage, and duplicate detection results. The report explicitly documents leakage risk from same-lesion multi-angle images and lists proxy grouping column options. All figures saved as PNGs in docs/figures/.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create dataset specification document from actual CSV headers</name>
  <files>
    docs/dataset_spec.md
  </files>
  <action>
    Create `docs/dataset_spec.md` using the ACTUAL column names from the downloaded dataset.csv (confirmed by Plan 02-01's output and the audit script's column printing).

    The document must contain these sections:

    **1. Header**
    ```markdown
    # BTXRD Dataset Specification

    *Dataset: Bone Tumor X-Ray Radiograph Dataset (BTXRD)*
    *Paper: Yao et al., "A Radiograph Dataset for the Classification, Localization, and Segmentation of Primary Bone Tumors" (Scientific Data, 2025)*
    *DOI: https://doi.org/10.1038/s41597-024-04311-y*
    *Data: https://doi.org/10.6084/m9.figshare.27865398*
    ```

    **2. License**
    ```markdown
    ## License

    **CC BY-NC-ND 4.0** (Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International)

    Source: Paper states CC BY-NC-ND 4.0 (Section "Usage Notes").

    > Note: The figshare metadata incorrectly lists CC BY 4.0. The paper is the authoritative source for licensing terms. This dataset may only be used for non-commercial purposes, and derivative datasets may not be redistributed.
    ```

    **3. Dataset Overview**
    - Total images: 3,746 JPEG radiographs
    - Source centers: 3 (details in provenance section)
    - Annotation format: LabelMe JSON (bounding boxes + segmentation masks, tumor images only)
    - CSV metadata: dataset.csv with 37 columns

    **4. Column Specification (All 37 Columns)**

    Create a comprehensive table documenting every column. Use the ACTUAL column names from the downloaded CSV (from `df.columns.tolist()` output). For each column:
    - Column name (exact, as in CSV header)
    - Type (int, str, float)
    - Description
    - Values/Range (e.g., "0 or 1", "1-88", "1, 2, or 3")
    - Notes (nullable? derived?)

    Group columns into subsections matching the research:
    - Identity and Demographics (image_id, center, age, gender)
    - Anatomical Site (15 binary columns)
    - Classification Labels (tumor, benign, malignant)
    - Tumor Subtypes (9 binary columns)
    - Body Region (3 binary columns)
    - Shooting Angle (3 binary columns)

    IMPORTANT: Use the EXACT column names as they appear in the actual CSV. If the research noted uncertainty about hyphens vs underscores vs spaces (e.g., "hip-bone" vs "hip_bone"), use whatever the actual CSV has. Read the 02-01-SUMMARY.md to find the confirmed column names. If the summary doesn't have them, run `python -c "import pandas as pd; print(pd.read_csv('data_raw/dataset.csv').columns.tolist())"` to get them.

    **5. Label Derivation Logic**
    ```markdown
    ## Label Derivation Logic

    The 3-class label used for classification is derived from the binary columns:

    | Priority | Condition | Label |
    |----------|-----------|-------|
    | 1 (highest) | `malignant == 1` | Malignant |
    | 2 | `benign == 1` | Benign |
    | 3 (default) | `tumor == 0` | Normal |

    ```python
    # Python implementation:
    if row["malignant"] == 1:
        label = "Malignant"
    elif row["benign"] == 1:
        label = "Benign"
    elif row["tumor"] == 0:
        label = "Normal"
    ```

    **Invariants:**
    - Every row where `tumor == 1` has either `benign == 1` or `malignant == 1` (never both, never neither)
    - Every row where `tumor == 0` has `benign == 0` and `malignant == 0`
    - Expected counts: Normal=1,879, Benign=1,525, Malignant=342 (total=3,746)
    ```

    **6. Data Provenance Per Center**
    ```markdown
    ## Data Provenance

    | Center | Source | Description | Normal | Benign | Malignant | Total | % |
    |--------|--------|-------------|--------|--------|-----------|-------|---|
    | 1 | Chinese hospitals | Guangxi Medical University, affiliated hospitals | 1,593 | 1,110 | 235 | 2,938 | 78.4% |
    | 2 | Radiopaedia.org | Online open-access radiology resource | 259 | 214 | 76 | 549 | 14.7% |
    | 3 | MedPix | US National Library of Medicine database | 27 | 201 | 31 | 259 | 6.9% |
    | **Total** | | | **1,879** | **1,525** | **342** | **3,746** | **100%** |
    ```

    **7. Annotation Format**
    - Format: LabelMe JSON
    - Coverage: Only tumor images (tumor=1) have annotations (~1,867 files)
    - Normal images (tumor=0) have NO annotations
    - Each JSON contains bounding box and segmentation polygon coordinates

    **8. Known Issues and Limitations**
    - No patient_id: same-lesion multi-angle images may exist, creating potential data leakage
    - Class imbalance: Malignant is 5.5x smaller than Normal
    - Some diagnoses are radiologist-empirical (no pathology confirmation)
    - Paper's baseline used random 80/20 split without patient-level grouping

    Write this file to `docs/dataset_spec.md`. The document should be factual, concise, and reference the paper for any claims. It should serve as a single-source-of-truth reference for anyone working with this dataset in subsequent phases.
  </action>
  <verify>
    File exists: `test -f docs/dataset_spec.md && echo "Spec exists"` passes.

    Has all required sections: `grep -c "^##" docs/dataset_spec.md` shows at least 6 section headers.

    License documented: `grep "CC BY-NC-ND 4.0" docs/dataset_spec.md` finds the license.

    Label derivation documented: `grep -i "malignant.*1.*Malignant" docs/dataset_spec.md` finds the derivation logic.

    Provenance documented: `grep -i "Radiopaedia\|MedPix\|Guangxi" docs/dataset_spec.md` finds center descriptions.

    Column count: `grep -c "|" docs/dataset_spec.md` is substantial (indicating tables are present).

    Figshare discrepancy noted: `grep -i "figshare.*CC BY" docs/dataset_spec.md` finds the license discrepancy note.
  </verify>
  <done>
    docs/dataset_spec.md documents all 37 columns using actual CSV header names, the 3-class label derivation logic (malignant=1 -> Malignant, benign=1 -> Benign, tumor=0 -> Normal), per-center data provenance (Centers 1-3 with sources and counts), the CC BY-NC-ND 4.0 license with figshare discrepancy note, annotation format, and known limitations including leakage risk.
  </done>
</task>

</tasks>

<verification>
1. `python scripts/audit.py --config configs/default.yaml` completes without errors
2. `docs/data_audit_report.md` exists and has sections: class distribution, image dimensions, per-center breakdown, missing values, annotation coverage, duplicate detection, leakage risk
3. `docs/figures/` contains at least 5 PNG figures
4. Class distribution in report confirms ~1,879 Normal / ~1,525 Benign / ~342 Malignant
5. Leakage risk section discusses same-lesion multi-angle images and proxy grouping columns
6. `docs/dataset_spec.md` exists and documents all 37 columns with actual CSV header names
7. dataset_spec.md contains label derivation logic, per-center provenance, and CC BY-NC-ND 4.0 license
8. `make audit` runs the audit script successfully
</verification>

<success_criteria>
- `make audit` generates docs/data_audit_report.md with embedded figures confirming class distribution (1,879/1,525/342)
- The audit report includes all 7 sections: class distribution, image dimensions, per-center breakdown, missing values, annotation coverage, duplicate detection, leakage risk assessment
- The leakage risk section documents the no-patient_id problem and lists proxy grouping column options
- docs/dataset_spec.md documents all 37 columns, label derivation logic, data provenance per center, and CC BY-NC-ND 4.0 license
- All figures render correctly when viewing the markdown on GitHub
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-acquisition-and-audit/02-02-SUMMARY.md`
</output>
