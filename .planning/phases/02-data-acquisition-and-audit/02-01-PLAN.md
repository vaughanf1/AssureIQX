---
phase: 02-data-acquisition-and-audit
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - configs/default.yaml
  - requirements.txt
  - scripts/download.py
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/download.py --config configs/default.yaml` downloads BTXRD.zip from figshare, verifies MD5, extracts, and organizes files into data_raw/"
    - "data_raw/images/ contains JPEG radiograph files"
    - "data_raw/dataset.csv exists and is a valid CSV with rows and 37 columns"
    - "data_raw/Annotations/ (or data_raw/annotations/) contains LabelMe JSON files for tumor images"
    - "Re-running the download script when data already exists skips the download with a message"
    - "MD5 verification catches corrupt downloads and raises a clear error"
  artifacts:
    - path: "scripts/download.py"
      provides: "Complete download, verify, extract, organize pipeline"
      contains: "download_with_verification"
    - path: "configs/default.yaml"
      provides: "Corrected figshare URL, MD5, article/file IDs, raw_dir=data_raw"
      contains: "expected_md5"
    - path: "requirements.txt"
      provides: "New dependencies: requests, imagehash"
      contains: "imagehash"
    - path: "data_raw/dataset.csv"
      provides: "Downloaded dataset CSV"
    - path: "data_raw/images/"
      provides: "Downloaded radiograph images"
  key_links:
    - from: "scripts/download.py"
      to: "configs/default.yaml"
      via: "load_config reads data.figshare_url, data.expected_md5, data.raw_dir"
      pattern: "cfg\\[.data.\\]"
    - from: "scripts/download.py"
      to: "data_raw/"
      via: "extracts ZIP contents into raw_dir from config"
      pattern: "raw_dir"
---

<objective>
Implement the download script that fetches the BTXRD dataset from figshare, verifies integrity via MD5, extracts the ZIP, and organizes files into `data_raw/`. Also fix the incorrect figshare URL in config and add missing dependencies.

Purpose: Without downloaded data, no audit, splitting, or training can happen. This is the critical first step of Phase 2 -- all subsequent plans depend on `data_raw/` being populated correctly.

Output: Working `scripts/download.py`, updated `configs/default.yaml` with correct figshare metadata, updated `requirements.txt` with `requests` and `imagehash`.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-acquisition-and-audit/02-RESEARCH.md
@.planning/phases/01-scaffold-and-infrastructure/01-02-SUMMARY.md
@configs/default.yaml
@scripts/download.py
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update config and dependencies, then implement download.py</name>
  <files>
    configs/default.yaml
    requirements.txt
    scripts/download.py
  </files>
  <action>
    **Step 1: Fix configs/default.yaml**

    Update the `data:` section to correct the figshare URL and add download metadata. Change these keys:

    ```yaml
    data:
      raw_dir: data_raw                # CHANGED from "data/raw" to match success criteria and .gitignore
      splits_dir: data/splits
      image_size: 224
      num_workers: 4
      train_split: 0.70
      val_split: 0.15
      test_split: 0.15
      figshare_url: "https://ndownloader.figshare.com/files/50653575"   # CHANGED: direct file URL (was wrong article ID)
      figshare_article_id: 27865398    # NEW
      figshare_file_id: 50653575       # NEW
      expected_md5: "59132a5036d030580ccade8add8f13df"   # NEW
      expected_image_count: 3746       # NEW
    ```

    Leave all other sections (model, training, evaluation, gradcam, inference, paths, seed, device) unchanged.

    **Step 2: Update requirements.txt**

    Add two new dependencies at the end of the file, under a new `# Download and audit` comment section:

    ```
    # Download and audit
    requests>=2.31.0
    imagehash==4.3.2
    ```

    `requests` is needed for streaming HTTP download with progress. `imagehash` is needed for perceptual duplicate detection in the audit script (Plan 02-02), but add it now so both plans share a single dependency update.

    **Step 3: Implement scripts/download.py**

    Replace the placeholder NotImplementedError with a complete implementation. Keep the existing structure (shebang, docstring, PROJECT_ROOT, argparse, load_config, set_seed) but replace the `raise NotImplementedError(...)` block with the actual download logic.

    The script must implement these functions:

    1. `check_existing_data(raw_dir: Path, expected_count: int) -> bool`
       - Return True if data_raw/ already has images/ dir with files, Annotations/ dir, and dataset.csv
       - Quick validation: check dataset.csv exists and images/ has at least 1 file
       - Print a summary of what's found if data exists
       - This prevents re-downloading when data is already present

    2. `download_with_verification(url: str, dest: Path, expected_md5: str) -> None`
       - Use `requests.get(url, stream=True, allow_redirects=True, timeout=600)` for streaming download
       - Show progress with `tqdm` (total from Content-Length header, unit="B", unit_scale=True)
       - Compute MD5 incrementally using `hashlib.md5()` with `.update(chunk)` during download
       - Use 8192-byte chunks
       - After download, compare MD5 hex digest against expected_md5
       - If mismatch: delete the corrupt file and raise ValueError with both hashes shown
       - Create parent directories with `mkdir(parents=True, exist_ok=True)`

    3. `extract_and_organize(zip_path: Path, dest_dir: Path) -> None`
       - Use `zipfile.ZipFile` to extract all contents to dest_dir
       - Detect single top-level wrapper directory (e.g., `BTXRD/`): check if all ZIP entries share a common first path component
       - If wrapper exists: move all contents up one level (using `shutil.move` for robustness -- handles non-empty dirs), then remove the empty wrapper with `rmdir()`
       - After extraction, verify expected structure exists:
         - `dest_dir / "images"` is a directory (case-insensitive check -- try "images" and "Images")
         - `dest_dir / "Annotations"` is a directory (try "Annotations" and "annotations")
         - `dest_dir / "dataset.csv"` is a file
       - If the annotations directory is "Annotations" (capital A), leave it as-is -- do NOT rename to lowercase (the actual ZIP name is authoritative)
       - Print counts: number of image files (*.jpg, *.jpeg, case-insensitive), number of JSON files, CSV row count

    4. `main()` function flow:
       - Parse args (existing --config and --override, no new args needed)
       - Load config, set seed
       - Resolve raw_dir as PROJECT_ROOT / cfg["data"]["raw_dir"]
       - Check if data already exists via `check_existing_data()`. If yes, print "Data already exists at {raw_dir}. Skipping download." and return
       - Create a temp download path: `raw_dir / "BTXRD.zip"`
       - Call `download_with_verification()` with cfg["data"]["figshare_url"], the temp path, and cfg["data"]["expected_md5"]
       - Call `extract_and_organize()` with the zip path and raw_dir
       - Delete the ZIP file after successful extraction to save disk space
       - Print final summary: image count, annotation count, CSV path

    **Imports needed at top of file** (after the existing ones):
    - `import hashlib`
    - `import shutil`
    - `import zipfile`
    - `import requests`
    - `from tqdm import tqdm`

    **Important implementation details:**
    - Use `logging` (via `from src.utils.logging import setup_logging`) for log messages, not just print
    - The timeout for the download request should be 600 seconds (10 min for 801MB on slow connections)
    - For JPEG globbing, use a helper that combines `*.jpg` and `*.jpeg` patterns (case-insensitive): `list(d.glob("*.[jJ][pP][gG]")) + list(d.glob("*.[jJ][pP][eE][gG]"))`
    - The script must work when called as `python scripts/download.py --config configs/default.yaml` from the project root
    - Do NOT import or use `imagehash` in this script (that's for audit.py)
  </action>
  <verify>
    Syntax check: `python -c "import ast; ast.parse(open('scripts/download.py').read()); print('download.py: valid syntax')"` passes.

    Config check: `python -c "from src.utils.config import load_config; c = load_config('configs/default.yaml'); assert c['data']['raw_dir'] == 'data_raw'; assert '50653575' in c['data']['figshare_url']; assert c['data']['expected_md5'] == '59132a5036d030580ccade8add8f13df'; print('Config OK')"` passes.

    Requirements check: `grep -c 'requests' requirements.txt` returns at least 1, `grep -c 'imagehash' requirements.txt` returns at least 1.

    Import check: `python -c "import requests; import imagehash; print('Dependencies importable')"` passes (after pip install).

    Dry-run check: `python scripts/download.py --help` prints usage without errors.

    NOTE: The actual download (801MB) will happen when `make download` or `python scripts/download.py --config configs/default.yaml` is run. The verify step does NOT trigger the download -- it only checks syntax, config, and imports.
  </verify>
  <done>
    configs/default.yaml has corrected figshare_url (ndownloader file URL), expected_md5, raw_dir=data_raw. requirements.txt includes requests>=2.31.0 and imagehash==4.3.2. scripts/download.py implements streaming download with tqdm progress bar, MD5 verification, ZIP extraction with wrapper directory flattening, idempotent skip-if-exists check, and structure verification. The script runs without errors on `--help` and will download the 801MB BTXRD.zip when executed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run the download and verify data integrity</name>
  <files>
    data_raw/images/
    data_raw/Annotations/
    data_raw/dataset.csv
  </files>
  <action>
    **Step 1: Install new dependencies**

    Run `pip install requests>=2.31.0 imagehash==4.3.2` to ensure the new dependencies are available.

    **Step 2: Execute the download**

    Run `python scripts/download.py --config configs/default.yaml` from the project root. This will:
    - Download BTXRD.zip (~801MB) from figshare with a progress bar
    - Verify MD5 checksum matches `59132a5036d030580ccade8add8f13df`
    - Extract and organize into `data_raw/`
    - Delete the ZIP file after extraction

    Monitor for:
    - Download progress bar completing to 100%
    - MD5 verification passing (no ValueError raised)
    - Structure verification printing image and annotation counts

    **Step 3: Verify data structure**

    After the download completes, verify:
    - `data_raw/images/` exists and contains JPEG files (expected ~3,746)
    - `data_raw/Annotations/` (or `data_raw/annotations/`) exists and contains JSON files (expected ~1,867)
    - `data_raw/dataset.csv` exists and has 37 columns and ~3,746 rows
    - No leftover BTXRD.zip file remains
    - No top-level wrapper directory (e.g., `data_raw/BTXRD/`) remains

    Run quick Python validation:
    ```python
    python -c "
    import pandas as pd
    from pathlib import Path
    raw = Path('data_raw')
    df = pd.read_csv(raw / 'dataset.csv')
    imgs = list((raw / 'images').glob('*.[jJ][pP][gG]')) + list((raw / 'images').glob('*.[jJ][pP][eE][gG]'))
    print(f'CSV: {len(df)} rows, {len(df.columns)} columns')
    print(f'Images: {len(imgs)}')
    print(f'Columns: {df.columns.tolist()}')
    "
    ```

    **Step 4: Verify idempotency**

    Run the download script again: `python scripts/download.py --config configs/default.yaml`
    It should print "Data already exists" and exit without re-downloading.

    **Step 5: Print actual column names**

    This is critical for Plan 02-02 (audit and dataset_spec). Print the actual CSV column names:
    ```python
    python -c "import pandas as pd; df = pd.read_csv('data_raw/dataset.csv'); print(df.columns.tolist()); print(f'{len(df.columns)} columns')"
    ```

    Save the output mentally -- these exact column names will be needed for the audit script and dataset_spec.md.
  </action>
  <verify>
    `ls data_raw/images/ | head -5` shows JPEG filenames.

    `ls data_raw/Annotations/ | head -5` or `ls data_raw/annotations/ | head -5` shows JSON filenames.

    `python -c "import pandas as pd; df = pd.read_csv('data_raw/dataset.csv'); assert len(df) >= 3700; assert len(df.columns) >= 35; print(f'{len(df)} rows, {len(df.columns)} cols -- OK')"` passes.

    `python -c "from pathlib import Path; imgs = list(Path('data_raw/images').glob('*.[jJ][pP][gG]')) + list(Path('data_raw/images').glob('*.[jJ][pP][eE][gG]')); assert len(imgs) >= 3700; print(f'{len(imgs)} images -- OK')"` passes.

    `test ! -f data_raw/BTXRD.zip && echo "ZIP cleaned up"` confirms no leftover ZIP.

    `python scripts/download.py --config configs/default.yaml 2>&1 | grep -i "already exists"` confirms idempotent skip.
  </verify>
  <done>
    data_raw/ contains images/ (~3,746 JPEGs), Annotations/ (~1,867 JSONs), and dataset.csv (37 columns, ~3,746 rows). MD5 integrity verified. ZIP cleaned up. Re-running the script skips download. Actual column names are known for use in Plan 02-02.
  </done>
</task>

</tasks>

<verification>
1. `python scripts/download.py --help` prints usage without errors
2. `data_raw/images/` contains ~3,746 JPEG files
3. `data_raw/dataset.csv` has 37 columns and ~3,746 rows
4. `data_raw/Annotations/` contains JSON annotation files
5. No BTXRD.zip or wrapper directory remains in data_raw/
6. Re-running download.py skips with "already exists" message
7. configs/default.yaml has correct figshare URL with file ID 50653575
8. requirements.txt includes requests and imagehash
</verification>

<success_criteria>
- `make download` (or `python scripts/download.py --config configs/default.yaml`) successfully downloads, verifies, and extracts the BTXRD dataset
- data_raw/images/, data_raw/Annotations/ (or annotations/), and data_raw/dataset.csv all exist with expected content
- configs/default.yaml has corrected figshare URL, expected_md5, and raw_dir=data_raw
- requirements.txt includes requests>=2.31.0 and imagehash==4.3.2
- Download is idempotent (skip if data already exists)
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-acquisition-and-audit/02-01-SUMMARY.md`
</output>
