---
phase: 05-evaluation
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/evaluation/bootstrap.py
  - scripts/eval.py
  - src/evaluation/__init__.py
autonomous: true

must_haves:
  truths:
    - "Bootstrap 95% confidence intervals are computed for macro AUC and per-class sensitivity and saved as JSON in each split's results directory"
    - "A comparison table shows side-by-side metrics for both split strategies plus BTXRD baseline, saved as both JSON and CSV"
    - "The generalization gap between stratified and center-holdout performance is computed and included in the comparison"
    - "BTXRD paper baseline (YOLOv8s-cls) is included with explicit caveats about comparison limitations"
  artifacts:
    - path: "src/evaluation/bootstrap.py"
      provides: "bootstrap_confidence_intervals() computing CIs for AUC and per-class sensitivity"
      exports: ["bootstrap_confidence_intervals"]
    - path: "results/stratified/bootstrap_ci.json"
      provides: "Bootstrap CIs for stratified split"
    - path: "results/center_holdout/bootstrap_ci.json"
      provides: "Bootstrap CIs for center-holdout split"
    - path: "results/comparison_table.json"
      provides: "Side-by-side metrics for both splits + baseline + generalization gap"
    - path: "results/comparison_table.csv"
      provides: "Human-readable comparison table"
  key_links:
    - from: "scripts/eval.py"
      to: "src/evaluation/bootstrap.py"
      via: "bootstrap_confidence_intervals called with prediction arrays"
      pattern: "bootstrap_confidence_intervals"
    - from: "scripts/eval.py"
      to: "results/comparison_table.json"
      via: "Aggregates per-split metrics into comparison after both splits complete"
      pattern: "comparison_table"
---

<objective>
Add bootstrap confidence intervals and the dual-split comparison with BTXRD baseline to the evaluation pipeline, completing EVAL-06, EVAL-07, and EVAL-08.

Purpose: Bootstrap CIs quantify uncertainty in metrics (critical with only 51 Malignant test samples in stratified split). The comparison table makes the generalization gap between stratified and center-holdout performance explicit, and contextualizes results against the paper's baseline.
Output: `bootstrap.py` module, updated `eval.py` producing bootstrap_ci.json per split and comparison_table.json/csv aggregating all results.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-evaluation/05-RESEARCH.md
@.planning/phases/05-evaluation/05-01-SUMMARY.md

@src/evaluation/bootstrap.py
@src/evaluation/metrics.py
@src/evaluation/visualization.py
@scripts/eval.py
@configs/default.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement bootstrap.py confidence interval module</name>
  <files>src/evaluation/bootstrap.py, src/evaluation/__init__.py</files>
  <action>
Replace the docstring-only stub in `src/evaluation/bootstrap.py` with the full implementation.

**Function: `bootstrap_confidence_intervals(y_true, y_pred, y_prob, class_names, n_iterations=1000, confidence_level=0.95, seed=42)`**

Implementation:
1. Create isolated RNG: `rng = np.random.RandomState(seed)`.
2. Initialize collectors: `auc_scores = []`, `sensitivity_scores = {i: [] for i in range(num_classes)}`.
3. Loop `n_iterations` times:
   a. Sample indices with replacement: `idx = rng.choice(n_samples, size=n_samples, replace=True)`.
   b. Extract bootstrap sample: `bt = y_true[idx]`, `bp = y_pred[idx]`, `bprob = y_prob[idx]`.
   c. **Class presence guard**: `if len(np.unique(bt)) < num_classes: continue` — skip degenerate samples where a class is missing entirely. This is critical for the stratified split where Malignant has only 51 samples.
   d. Compute macro AUC: `roc_auc_score(bt, bprob, multi_class='ovr', average='macro')` wrapped in try/except ValueError (skip on failure).
   e. Compute per-class sensitivity: `recall_score(bt, bp, average=None, labels=list(range(num_classes)), zero_division=0)` — append each class's value to its collector.
4. Compute CIs using percentile method:
   - `alpha = (1 - confidence_level) / 2`
   - For each metric array: `mean`, `ci_lower = np.percentile(arr, alpha * 100)`, `ci_upper = np.percentile(arr, (1 - alpha) * 100)`, `n_valid = len(arr)`
5. Return dict structure:
   ```python
   {
       "macro_auc": {"mean": float, "ci_lower": float, "ci_upper": float, "n_valid": int},
       "sensitivity_Normal": {"mean": float, "ci_lower": float, "ci_upper": float, "n_valid": int},
       "sensitivity_Benign": {"mean": float, "ci_lower": float, "ci_upper": float, "n_valid": int},
       "sensitivity_Malignant": {"mean": float, "ci_lower": float, "ci_upper": float, "n_valid": int},
   }
   ```

Imports: `numpy`, `sklearn.metrics.roc_auc_score`, `sklearn.metrics.recall_score`.

**Update `src/evaluation/__init__.py`** to also export `bootstrap_confidence_intervals`.
  </action>
  <verify>
Run `python -c "from src.evaluation.bootstrap import bootstrap_confidence_intervals; print('Bootstrap import OK')"` from project root -- must succeed.
  </verify>
  <done>bootstrap.py contains bootstrap_confidence_intervals() that computes 95% CIs for macro AUC and per-class sensitivity with class-presence guard and reproducible RNG.</done>
</task>

<task type="auto">
  <name>Task 2: Add bootstrap CIs and comparison table to eval.py</name>
  <files>scripts/eval.py</files>
  <action>
Extend the eval.py script (implemented in Plan 05-01) to add bootstrap CI computation and the comparison table. This adds to the existing per-split loop and appends a post-loop comparison step.

**Within the per-split loop (after metrics computation and plot generation), add:**

1. **Bootstrap CIs** — After computing metrics for each split:
   ```python
   from src.evaluation.bootstrap import bootstrap_confidence_intervals

   ci_results = bootstrap_confidence_intervals(
       y_true, y_pred, y_prob, class_names,
       n_iterations=cfg["evaluation"]["bootstrap_iterations"],  # 1000
       confidence_level=cfg["evaluation"]["confidence_level"],  # 0.95
       seed=cfg.get("seed", 42),
   )
   ```
   Save to `results_dir / "bootstrap_ci.json"` using `json.dump` with the JSON serialization helper.
   Log: `"Bootstrap CIs (1000 iterations): Macro AUC = {mean:.3f} [{lower:.3f}, {upper:.3f}], Malignant Sensitivity = {mean:.3f} [{lower:.3f}, {upper:.3f}]"`.

2. **Collect per-split results** — Store metrics_summary and bootstrap CIs in a dict keyed by split name (`"stratified"`, `"center_holdout"`) for comparison table generation after the loop.

**After the per-split loop completes, add:**

3. **BTXRD baseline data** — Hardcode the paper's results (from RESEARCH.md, verified from PMC11739492):
   ```python
   BTXRD_BASELINE = {
       "model": "YOLOv8s-cls",
       "source": "Yao et al., Scientific Data 2025 (PMC11739492)",
       "split": "Random 80/20 (no patient grouping)",
       "image_size": 600,
       "epochs": 300,
       "per_class_precision": {"Normal": 0.913, "Benign": 0.881, "Malignant": 0.734},
       "per_class_recall": {"Normal": 0.898, "Benign": 0.875, "Malignant": 0.839},
       "caveats": [
           "Random 80/20 split without patient-level grouping (potential data leakage)",
           "Validation set used for reporting (no separate held-out test set)",
           "Different image size (600px vs 224px)",
           "Different architecture (YOLOv8s-cls vs EfficientNet-B0)",
           "Different training duration (300 epochs vs early stopping at ~5 epochs)",
           "Paper reports mAP@0.5 (detection metric) which is not directly comparable to AUC"
       ],
   }
   ```

4. **Comparison table JSON** — Build `comparison_table.json` at `results/comparison_table.json`:
   ```python
   comparison = {
       "stratified": collected_results["stratified"],
       "center_holdout": collected_results["center_holdout"],
       "btxrd_baseline": BTXRD_BASELINE,
       "generalization_gap": {
           "description": "center_holdout minus stratified (negative means center-holdout is worse)",
           "macro_auc_gap": center_auc - stratified_auc,
           "malignant_sensitivity_gap": center_mal_sens - stratified_mal_sens,
           "accuracy_gap": center_acc - stratified_acc,
       },
   }
   ```
   Each split's entry in the comparison should include: macro_auc, per_class_auc, per_class_sensitivity, per_class_specificity, per_class_precision (from classification report), per_class_recall (from classification report), per_class_f1 (from classification report), accuracy, macro_f1, malignant_sensitivity, test_set_size, and bootstrap_ci summary (mean + CI for macro_auc and malignant_sensitivity).

5. **Comparison table CSV** — Save a simplified CSV at `results/comparison_table.csv` using pandas DataFrame:
   Rows: one per metric (Macro AUC, Accuracy, Normal Sensitivity, Benign Sensitivity, Malignant Sensitivity, Normal Specificity, Benign Specificity, Malignant Specificity, Normal F1, Benign F1, Malignant F1, Macro F1)
   Columns: Metric, Stratified, Center Holdout, BTXRD Baseline (where available, else "-"), Gap

6. **Log the comparison summary** — Print a formatted table to stdout showing the key metrics side-by-side.

**Import additions at top of eval.py:**
```python
from src.evaluation.bootstrap import bootstrap_confidence_intervals
import pandas as pd
```
  </action>
  <verify>
Run `python scripts/eval.py --config configs/default.yaml` from project root. Verify:
1. `results/stratified/bootstrap_ci.json` exists with macro_auc and sensitivity_Malignant entries, each having mean/ci_lower/ci_upper/n_valid
2. `results/center_holdout/bootstrap_ci.json` exists with same structure
3. `results/comparison_table.json` exists with stratified, center_holdout, btxrd_baseline, and generalization_gap sections
4. `results/comparison_table.csv` exists and is a valid CSV with Metric/Stratified/Center Holdout/BTXRD Baseline/Gap columns
5. n_valid in bootstrap CIs is close to 1000 (may be slightly less if some samples had missing classes)
6. Console output shows comparison summary
  </verify>
  <done>Running `make evaluate` produces bootstrap_ci.json in each split's results directory, plus comparison_table.json and comparison_table.csv at the results root. The comparison includes both splits, BTXRD baseline with caveats, and the generalization gap.</done>
</task>

</tasks>

<verification>
1. `python scripts/eval.py --config configs/default.yaml` runs without errors
2. Full output file inventory:
   - `results/stratified/`: roc_curves.png, pr_curves.png, confusion_matrix.png, confusion_matrix_normalized.png, metrics_summary.json, classification_report.json, bootstrap_ci.json (7 files)
   - `results/center_holdout/`: same 7 files
   - `results/`: comparison_table.json, comparison_table.csv (2 files)
3. `python -c "import json; d=json.load(open('results/stratified/bootstrap_ci.json')); print(f'Macro AUC CI: [{d[\"macro_auc\"][\"ci_lower\"]:.3f}, {d[\"macro_auc\"][\"ci_upper\"]:.3f}]')"` prints valid CI bounds
4. `python -c "import json; d=json.load(open('results/comparison_table.json')); print(f'Gap: {d[\"generalization_gap\"][\"macro_auc_gap\"]:.3f}')"` prints the generalization gap
5. `cat results/comparison_table.csv` shows a readable metrics comparison table
</verification>

<success_criteria>
- Bootstrap CIs computed with 1000 iterations for macro AUC and per-class sensitivity (EVAL-07)
- n_valid reported and close to 1000 for both splits
- comparison_table.json contains stratified, center_holdout, btxrd_baseline, and generalization_gap (EVAL-06, EVAL-08)
- BTXRD baseline includes all 7 caveats about comparison limitations (EVAL-08)
- Generalization gap is explicit (center_holdout minus stratified for key metrics)
- `make evaluate` runs the full pipeline end-to-end successfully
</success_criteria>

<output>
After completion, create `.planning/phases/05-evaluation/05-02-SUMMARY.md`
</output>
