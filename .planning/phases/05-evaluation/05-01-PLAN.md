---
phase: 05-evaluation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/evaluation/metrics.py
  - src/evaluation/visualization.py
  - scripts/eval.py
  - src/evaluation/__init__.py
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/eval.py` produces ROC curve PNGs, PR curve PNGs, confusion matrix heatmaps (absolute + normalized), and classification report JSON in each split's results directory"
    - "Per-class sensitivity and specificity are computed and written to metrics_summary.json with Malignant sensitivity as the headline metric"
    - "All plots are publication-quality with labeled axes, titles, and legends"
  artifacts:
    - path: "src/evaluation/metrics.py"
      provides: "compute_all_metrics() returning ROC data, PR data, sensitivity, specificity, confusion matrix, classification report"
      exports: ["compute_all_metrics", "run_inference"]
    - path: "src/evaluation/visualization.py"
      provides: "plot_roc_curves(), plot_pr_curves(), plot_confusion_matrices() saving PNGs"
      exports: ["plot_roc_curves", "plot_pr_curves", "plot_confusion_matrices"]
    - path: "scripts/eval.py"
      provides: "CLI entry point orchestrating inference -> metrics -> plots for each split strategy"
  key_links:
    - from: "scripts/eval.py"
      to: "src/evaluation/metrics.py"
      via: "run_inference + compute_all_metrics"
      pattern: "run_inference.*compute_all_metrics"
    - from: "scripts/eval.py"
      to: "src/evaluation/visualization.py"
      via: "plot functions called with metrics dict"
      pattern: "plot_roc_curves.*plot_pr_curves.*plot_confusion_matrices"
    - from: "scripts/eval.py"
      to: "src/models/factory.py"
      via: "load_checkpoint + create_model for model loading"
      pattern: "load_checkpoint.*create_model"
---

<objective>
Implement the core evaluation pipeline: metrics computation module, visualization module, and the eval.py orchestration script that loads trained models, runs inference on test sets, computes all classification metrics (EVAL-01 through EVAL-05), and generates publication-quality plots.

Purpose: This is the foundation of Phase 5 -- all downstream work (bootstrap CIs, comparison tables) consumes the metrics and prediction arrays produced here.
Output: Working `metrics.py`, `visualization.py`, and `scripts/eval.py` that produce per-split result directories with ROC curves, PR curves, confusion matrices, classification reports, and a metrics summary JSON.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-evaluation/05-RESEARCH.md

@src/evaluation/metrics.py
@src/evaluation/visualization.py
@src/evaluation/__init__.py
@scripts/eval.py
@src/models/factory.py
@src/data/dataset.py
@src/data/transforms.py
@configs/default.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement metrics.py and visualization.py</name>
  <files>src/evaluation/metrics.py, src/evaluation/visualization.py, src/evaluation/__init__.py</files>
  <action>
Replace the docstring-only stubs with full implementations.

**src/evaluation/metrics.py** — Two functions:

1. `run_inference(model, dataloader, device)` — Run model inference on full test set:
   - `model.eval()` + `torch.no_grad()` context
   - Iterate over dataloader, compute `F.softmax(logits, dim=1)` (model outputs raw logits, softmax applied exactly ONCE here)
   - Collect labels via `labels.numpy()`, probs via `probs.cpu().numpy()`
   - Return `(y_true, y_pred, y_prob)` as numpy arrays: `y_true` shape `(N,)` int64, `y_pred` shape `(N,)` int64 (argmax of y_prob), `y_prob` shape `(N, C)` float
   - Use `tqdm` for progress display

2. `compute_all_metrics(y_true, y_pred, y_prob, class_names)` — Compute all EVAL-01 through EVAL-05 metrics:
   - EVAL-01: Per-class ROC curves via `roc_curve` on `label_binarize` columns + per-class AUC via `auc()`, macro AUC via `roc_auc_score(y_true, y_prob, multi_class='ovr', average='macro')`
   - EVAL-02: Per-class PR curves via `precision_recall_curve` on binarized columns + AP via `average_precision_score`
   - EVAL-03: Sensitivity and specificity via `multilabel_confusion_matrix` — extract TN/FP/FN/TP, compute `sensitivity = tp/(tp+fn)`, `specificity = tn/(tn+fp)`
   - EVAL-04: Raw confusion matrix via `confusion_matrix(y_true, y_pred)`
   - EVAL-05: Classification report via `classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)`
   - Return dict with keys: `roc_fpr`, `roc_tpr`, `roc_auc` (dict class_name->float), `macro_auc`, `pr_precision`, `pr_recall`, `average_precision` (dict class_name->float), `sensitivity` (dict class_name->float), `specificity` (dict class_name->float), `confusion_matrix` (ndarray), `classification_report` (dict), `malignant_sensitivity` (float), `accuracy` (float)
   - Label ordering: Normal=0, Benign=1, Malignant=2. Use `classes=list(range(num_classes))` for `label_binarize`.

Imports: `sklearn.metrics` (roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score, confusion_matrix, multilabel_confusion_matrix, classification_report), `sklearn.preprocessing.label_binarize`, `numpy`, `torch`, `torch.nn.functional`, `tqdm`.

**src/evaluation/visualization.py** — Three functions:

1. `plot_roc_curves(metrics, class_names, output_path)` — One-vs-rest ROC + macro on single figure:
   - Per-class curves with colors `['#2196F3', '#FF9800', '#F44336']` (Blue/Orange/Red for Normal/Benign/Malignant)
   - Macro-average as navy dashed line (interpolate per-class TPR to common FPR grid of 1000 points)
   - Diagonal reference line (black dashed, alpha=0.3)
   - Legend with per-class AUC values formatted `.3f`, axis labels, title "ROC Curves (One-vs-Rest)", grid alpha=0.3
   - `figsize=(8, 8)`, `dpi=150`, `fig.tight_layout()`, `plt.close(fig)` after save

2. `plot_pr_curves(metrics, class_names, output_path)` — Per-class PR curves:
   - Same color scheme as ROC
   - Legend with per-class AP values formatted `.3f`
   - Axis labels (Recall / Precision), title "Precision-Recall Curves (One-vs-Rest)", grid
   - `figsize=(8, 8)`, `dpi=150`, close after save

3. `plot_confusion_matrices(cm, class_names, output_dir)` — Two separate heatmaps saved:
   - `confusion_matrix.png`: Absolute counts, `sns.heatmap(annot=True, fmt='d', cmap='Blues', square=True)`, labeled axes "True Label" / "Predicted Label"
   - `confusion_matrix_normalized.png`: Row-normalized (`cm / cm.sum(axis=1)[:, np.newaxis]`), `fmt='.2f'`, `vmin=0, vmax=1`
   - Both: `figsize=(8, 6)`, `linewidths=0.5`, `cbar_kws={'shrink': 0.8}`, `dpi=150`, close after save

All visualization functions MUST start with `matplotlib.use('Agg')` before pyplot import (or use the module-level guard). Never call `plt.show()`.

**src/evaluation/__init__.py** — Update to export key functions:
```python
from src.evaluation.metrics import compute_all_metrics, run_inference
from src.evaluation.visualization import plot_roc_curves, plot_pr_curves, plot_confusion_matrices
```
  </action>
  <verify>
Run `python -c "from src.evaluation.metrics import run_inference, compute_all_metrics; from src.evaluation.visualization import plot_roc_curves, plot_pr_curves, plot_confusion_matrices; print('Imports OK')"` from project root -- must print "Imports OK" without errors.
  </verify>
  <done>metrics.py contains run_inference() and compute_all_metrics() with all EVAL-01 through EVAL-05 computations. visualization.py contains plot_roc_curves(), plot_pr_curves(), and plot_confusion_matrices(). All imports resolve cleanly.</done>
</task>

<task type="auto">
  <name>Task 2: Implement eval.py orchestration script</name>
  <files>scripts/eval.py</files>
  <action>
Replace the NotImplementedError placeholder in `scripts/eval.py` with the full evaluation orchestration logic. Follow the existing script template pattern (shebang, docstring, PROJECT_ROOT, argparse, config+seed).

The main() function must:

1. **Parse args and load config** (already scaffolded — keep existing argparse setup).

2. **Set up logging** — `logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')`.

3. **Define split mapping** — Map config split names to directory names and checkpoint paths:
   ```python
   SPLIT_MAP = {
       "stratified": {"dir_name": "stratified", "checkpoint": "best_stratified.pt", "test_csv": "stratified_test.csv"},
       "center": {"dir_name": "center_holdout", "checkpoint": "best_center.pt", "test_csv": "center_test.csv"},
   }
   ```

4. **Loop over each split strategy** in `cfg["evaluation"]["split_strategies"]` (default: ["stratified", "center"]):

   a. Load checkpoint via `load_checkpoint(checkpoint_path, device="cpu")`, extract config, class_names.
   b. Create model via `create_model(ckpt_config)`, load state dict, move to device, set eval mode.
   c. Create test dataset via `BTXRDDataset(test_csv_path, images_dir, get_test_transforms(cfg["data"]["image_size"]))`, create dataloader via `create_dataloader(test_dataset, batch_size=cfg["training"]["batch_size"], shuffle=False, num_workers=cfg["data"]["num_workers"])`.
   d. Run inference via `run_inference(model, dataloader, device)` to get `(y_true, y_pred, y_prob)`.
   e. Compute metrics via `compute_all_metrics(y_true, y_pred, y_prob, class_names)`.
   f. Generate all plots:
      - `plot_roc_curves(metrics, class_names, results_dir / "roc_curves.png")`
      - `plot_pr_curves(metrics, class_names, results_dir / "pr_curves.png")`
      - `plot_confusion_matrices(metrics["confusion_matrix"], class_names, results_dir)`
   g. Save metrics summary JSON to `results_dir / "metrics_summary.json"` containing: macro_auc, per-class AUC, per-class AP, per-class sensitivity, per-class specificity, malignant_sensitivity, accuracy, and test set size.
   h. Save classification report JSON to `results_dir / "classification_report.json"`.
   i. Log headline metrics: Malignant sensitivity, macro AUC, accuracy.

5. **Path construction:**
   - Checkpoint: `PROJECT_ROOT / cfg["paths"]["checkpoints_dir"] / split_info["checkpoint"]`
   - Test CSV: `PROJECT_ROOT / cfg["data"]["splits_dir"] / split_info["test_csv"]`
   - Images dir: `PROJECT_ROOT / cfg["data"]["raw_dir"] / "images"`
   - Results dir: `PROJECT_ROOT / cfg["paths"]["results_dir"] / split_info["dir_name"]`
   - Create results_dir with `mkdir(parents=True, exist_ok=True)`

6. **Device:** Use `get_device(cfg.get("device", "auto"))`.

7. **Imports at top (after sys.path):**
   ```python
   from src.evaluation.metrics import run_inference, compute_all_metrics
   from src.evaluation.visualization import plot_roc_curves, plot_pr_curves, plot_confusion_matrices
   from src.models.factory import create_model, load_checkpoint, get_device
   from src.data.dataset import BTXRDDataset, create_dataloader
   from src.data.transforms import get_test_transforms
   ```

8. **JSON serialization helper** — metrics contain numpy types. Create a small helper:
   ```python
   def _json_serializable(obj):
       if isinstance(obj, np.integer): return int(obj)
       if isinstance(obj, np.floating): return float(obj)
       if isinstance(obj, np.ndarray): return obj.tolist()
       raise TypeError(f"Type {type(obj)} not serializable")
   ```
   Use `json.dump(data, f, indent=2, default=_json_serializable)`.

DO NOT implement bootstrap CIs or comparison table in this plan -- those are Plan 05-02. This plan ONLY produces per-split results.
  </action>
  <verify>
Run `python scripts/eval.py --config configs/default.yaml` from the project root. Verify:
1. No errors/exceptions during execution
2. `results/stratified/` contains: roc_curves.png, pr_curves.png, confusion_matrix.png, confusion_matrix_normalized.png, metrics_summary.json, classification_report.json
3. `results/center_holdout/` contains the same 6 files
4. `cat results/stratified/metrics_summary.json` shows valid JSON with macro_auc, per-class sensitivity, malignant_sensitivity keys
5. PNG files are non-empty (> 10KB each)
  </verify>
  <done>Running `python scripts/eval.py` produces per-split result directories each containing ROC curves, PR curves, confusion matrix heatmaps (absolute + normalized), classification report JSON, and metrics summary JSON. Malignant sensitivity is logged as the headline metric.</done>
</task>

</tasks>

<verification>
1. `python scripts/eval.py --config configs/default.yaml` runs without errors for both splits
2. `ls results/stratified/` shows: roc_curves.png, pr_curves.png, confusion_matrix.png, confusion_matrix_normalized.png, metrics_summary.json, classification_report.json
3. `ls results/center_holdout/` shows same 6 files
4. `python -c "import json; d=json.load(open('results/stratified/metrics_summary.json')); print(f'Macro AUC: {d[\"macro_auc\"]:.3f}, Malignant Sensitivity: {d[\"malignant_sensitivity\"]:.3f}')"` prints reasonable values
5. All PNG files are > 10KB (not empty/corrupted)
</verification>

<success_criteria>
- Per-split result directories exist with all 6 required files each (EVAL-01 through EVAL-05 covered)
- metrics_summary.json contains macro_auc, per-class AUC, per-class sensitivity, per-class specificity, malignant_sensitivity, and accuracy
- ROC curves show per-class + macro-average with AUC values in legend
- PR curves show per-class curves with AP values in legend
- Confusion matrices show both absolute counts and row-normalized versions
- Classification report contains precision, recall, F1 per class
</success_criteria>

<output>
After completion, create `.planning/phases/05-evaluation/05-01-SUMMARY.md`
</output>
