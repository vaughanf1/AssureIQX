---
phase: 04-model-training
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - scripts/train.py
  - Makefile
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/train.py --config configs/default.yaml` trains an EfficientNet-B0 on the stratified split with weighted cross-entropy loss and early stopping"
    - "Training uses WEIGHTED CrossEntropyLoss for training and UNWEIGHTED CrossEntropyLoss for validation (early stopping signal)"
    - "Best checkpoint (lowest val_loss) and final checkpoint are saved to checkpoints/"
    - "Training log CSV records epoch, train_loss, val_loss, val_acc, val_recall_normal, val_recall_benign, val_recall_malignant, lr"
    - "Loss curve PNG is saved showing train and val loss convergence"
    - "Per-class recall is logged every epoch to detect Malignant class collapse"
    - "`make train-all` runs training for both stratified and center split strategies sequentially"
  artifacts:
    - path: "scripts/train.py"
      provides: "Complete training script with data loading, training loop, validation, early stopping, logging, and plotting"
      min_lines: 150
    - path: "Makefile"
      provides: "train-all target for both split strategies"
      contains: "train-all"
  key_links:
    - from: "scripts/train.py"
      to: "src/models/factory.py"
      via: "imports create_model, compute_class_weights, get_device, save_checkpoint, EarlyStopping"
      pattern: "from src\\.models"
    - from: "scripts/train.py"
      to: "src/data/dataset.py"
      via: "imports BTXRDDataset, create_dataloader"
      pattern: "from src\\.data\\.dataset import"
    - from: "scripts/train.py"
      to: "src/data/transforms.py"
      via: "imports get_train_transforms, get_val_transforms"
      pattern: "from src\\.data\\.transforms import"
    - from: "scripts/train.py"
      to: "configs/default.yaml"
      via: "load_config reads training hyperparameters"
      pattern: "load_config"
---

<objective>
Implement the complete training script (`scripts/train.py`) with the full training loop, validation, early stopping, checkpoint saving, CSV logging, and loss curve plotting. Also add `make train-all` Makefile target.

Purpose: This is the core training pipeline that produces trained model checkpoints. It wires together the model architecture (Plan 04-01), data loaders (Phase 3), and configuration system (Phase 1) into a runnable training script.

Output: A working `scripts/train.py` that trains EfficientNet-B0 on BTXRD and produces checkpoints + logs, plus updated Makefile.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-model-training/04-RESEARCH.md
@.planning/phases/04-model-training/04-01-SUMMARY.md
@configs/default.yaml
@scripts/train.py
@src/models/classifier.py
@src/models/factory.py
@src/data/dataset.py
@src/data/transforms.py
@src/utils/config.py
@src/utils/reproducibility.py
@Makefile
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement the complete training script</name>
  <files>scripts/train.py</files>
  <action>
Replace the NotImplementedError stub in `scripts/train.py` with the full training implementation. The script structure:

**Imports:**
- `torch`, `torch.nn`, `torch.optim`, `torch.optim.lr_scheduler`
- `tqdm` for progress bars
- `csv` for training log writing
- `matplotlib.pyplot` for loss curve plots
- `sklearn.metrics.recall_score` for per-class recall
- `logging`, `time` for logging and timing
- From project: `load_config`, `set_seed`, `BTXRDDataset`, `create_dataloader`, `get_train_transforms`, `get_val_transforms`, `create_model`, `compute_class_weights`, `get_device`, `save_checkpoint`, `EarlyStopping`

**`train_one_epoch(model, loader, criterion, optimizer, device)` function:**
- Sets `model.train()`
- Iterates over loader with tqdm progress bar
- For each batch: move images/labels to device, zero_grad, forward, loss, backward, step
- Accumulates running_loss (weighted by batch size) and correct/total counts
- Returns `(epoch_loss, epoch_acc)` where `epoch_loss = running_loss / total`

**`validate(model, loader, criterion, device, num_classes=3)` function:**
- Sets `model.eval()`
- `torch.no_grad()` context
- Iterates over loader, accumulates loss and collects all predictions/labels
- Computes `sklearn.metrics.recall_score(all_labels, all_preds, average=None, zero_division=0)` for per-class recall
- Computes overall accuracy
- Returns `(epoch_loss, epoch_acc, per_class_recall)` -- per_class_recall is numpy array of shape (3,)

**`save_training_log(log_rows, filepath)` function:**
- Writes CSV with fieldnames: `epoch`, `train_loss`, `val_loss`, `val_acc`, `val_recall_normal`, `val_recall_benign`, `val_recall_malignant`, `lr`
- Creates parent directory

**`plot_loss_curves(log_rows, filepath)` function:**
- Plots train_loss and val_loss vs epoch on same axes
- Labels, grid, legend, title "Training and Validation Loss"
- Saves as PNG at 150 DPI
- Also creates a second subplot or separate figure showing per-class recall vs epoch (3 lines: Normal, Benign, Malignant)
- Saves recall plot as `recall_curve.png` in the same results directory

**`main()` function flow (replacing the NotImplementedError):**

1. Parse args and load config (KEEP existing argparse and config loading code)
2. Set seed (KEEP existing set_seed call)
3. Resolve device: `device = get_device(cfg.get("device", "auto"))`
4. Determine split strategy: `split_strategy = cfg["training"]["split_strategy"]` -- "stratified" or "center"
5. Map split_strategy to file prefix: "stratified" -> "stratified", "center" -> "center"
6. Build file paths:
   - `train_csv = PROJECT_ROOT / cfg["data"]["splits_dir"] / f"{split_prefix}_train.csv"`
   - `val_csv = PROJECT_ROOT / cfg["data"]["splits_dir"] / f"{split_prefix}_val.csv"`
   - `images_dir = PROJECT_ROOT / cfg["data"]["raw_dir"] / "images"`
   - `results_dir = PROJECT_ROOT / cfg["paths"]["results_dir"] / ("stratified" if split_strategy == "stratified" else "center_holdout")`
   - `checkpoints_dir = PROJECT_ROOT / cfg["paths"]["checkpoints_dir"]`
   - Create both dirs with `mkdir(parents=True, exist_ok=True)`
7. Load datasets:
   - `train_dataset = BTXRDDataset(train_csv, images_dir, get_train_transforms(image_size))`
   - `val_dataset = BTXRDDataset(val_csv, images_dir, get_val_transforms(image_size))`
   - `image_size = cfg["data"]["image_size"]`
8. Create data loaders:
   - `train_loader = create_dataloader(train_dataset, batch_size, shuffle=True, num_workers=cfg["data"]["num_workers"])`
   - `val_loader = create_dataloader(val_dataset, batch_size, shuffle=False, num_workers=cfg["data"]["num_workers"])`
   - `batch_size = cfg["training"]["batch_size"]`
9. Create model: `model = create_model(cfg).to(device)`
10. Compute class weights from TRAINING set only: `class_weights = compute_class_weights(train_dataset.labels, cfg["model"]["num_classes"]).to(device)`
11. Create loss functions:
    - `train_criterion = torch.nn.CrossEntropyLoss(weight=class_weights)` -- WEIGHTED for training
    - `val_criterion = torch.nn.CrossEntropyLoss()` -- UNWEIGHTED for validation/early stopping
12. Create optimizer: `AdamW(model.parameters(), lr=cfg["training"]["learning_rate"], weight_decay=cfg["training"]["weight_decay"])`
13. Create scheduler: `CosineAnnealingLR(optimizer, T_max=cfg["training"]["epochs"], eta_min=1e-6)`
14. Create early stopping: `EarlyStopping(patience=cfg["training"]["early_stopping_patience"])`
15. Training loop (`for epoch in range(1, epochs + 1)`):
    a. `train_loss, train_acc = train_one_epoch(model, train_loader, train_criterion, optimizer, device)`
    b. `val_loss, val_acc, per_class_recall = validate(model, val_loader, val_criterion, device)`
    c. `scheduler.step()`
    d. Get current LR: `current_lr = optimizer.param_groups[0]["lr"]`
    e. Build log_row dict with all metrics (round floats to 6 decimal places)
    f. Append to log_rows list
    g. Print epoch summary: epoch, train_loss, val_loss, val_acc, per-class recall, lr
    h. If `val_loss < best_val_loss`: update best_val_loss, save best checkpoint to `checkpoints_dir / f"best_{split_prefix}.pt"`, print "Saved best checkpoint"
    i. Check early stopping: if `early_stopping(val_loss)`: print "Early stopping triggered at epoch {epoch}", break
    j. Log WARNING if Malignant recall (per_class_recall[2]) == 0 for 2+ consecutive epochs
16. After loop:
    a. Save final checkpoint to `checkpoints_dir / f"final_{split_prefix}.pt"`
    b. Save training log CSV to `results_dir / "training_log.csv"`
    c. Plot and save loss curves to `results_dir / "loss_curve.png"`
    d. Plot and save recall curves to `results_dir / "recall_curve.png"`
    e. Print training summary: total epochs, best val_loss, best epoch, final per-class recall, checkpoint paths

**Key constraints (from research):**
- WEIGHTED loss for training ONLY. UNWEIGHTED loss for validation (drives early stopping). This is CRITICAL.
- Class weights computed from TRAINING set labels only (not full dataset).
- Class weight tensor must be moved to device before passing to CrossEntropyLoss.
- `model.eval()` before validation, `model.train()` before training (dropout behavior).
- `torch.no_grad()` during validation.
- Use `matplotlib.use("Agg")` at top of script to avoid display backend issues.
- split_prefix naming: "stratified" -> checkpoint names `best_stratified.pt`, `final_stratified.pt`; "center" -> `best_center.pt`, `final_center.pt`
  </action>
  <verify>
Run a dry-run verification (do NOT actually train -- just verify the script loads and can set up everything):

```bash
python -c "
import sys; sys.path.insert(0, '.')
import ast

# Parse the script to check it has required functions
with open('scripts/train.py') as f:
    tree = ast.parse(f.read())

funcs = [n.name for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]
required = ['main', 'train_one_epoch', 'validate', 'save_training_log', 'plot_loss_curves']
for fn in required:
    assert fn in funcs, f'Missing function: {fn}'
    print(f'Found: {fn}')

# Check imports work
from scripts.train import train_one_epoch, validate, save_training_log, plot_loss_curves
print('All imports OK')
"
```

Also verify the script can be invoked with `--help`:
```bash
python scripts/train.py --help
```

Must show usage with --config and --override args.
  </verify>
  <done>scripts/train.py is a complete training script with train_one_epoch, validate, early stopping, checkpoint saving, CSV logging, and loss curve plotting. It uses weighted loss for training and unweighted loss for validation. All functions are importable and the script runs with --help.</done>
</task>

<task type="auto">
  <name>Task 2: Add train-all Makefile target</name>
  <files>Makefile</files>
  <action>
Add a `train-all` target to the Makefile that trains both split strategies sequentially. Insert it right after the existing `train` target.

Add to `.PHONY` line: `train-all`

Add the target:
```makefile
train-all: ## Train on both stratified and center-holdout splits
	python scripts/train.py --config configs/default.yaml --override training.split_strategy=stratified
	python scripts/train.py --config configs/default.yaml --override training.split_strategy=center
```

This keeps `make train` as the default (stratified only, from config) and adds `make train-all` for running both.
  </action>
  <verify>
Run: `make help` and confirm both `train` and `train-all` targets appear in the output.
  </verify>
  <done>Makefile has `train` (single strategy from config) and `train-all` (both strategies sequentially) targets.</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. `python scripts/train.py --help` prints usage with --config and --override
2. `make help` shows both `train` and `train-all` targets
3. `scripts/train.py` contains all required functions: main, train_one_epoch, validate, save_training_log, plot_loss_curves
4. The script imports from src.models (create_model, compute_class_weights, get_device, save_checkpoint, EarlyStopping) and src.data (BTXRDDataset, create_dataloader, get_train_transforms, get_val_transforms)
5. Weighted CrossEntropyLoss is used for training, unweighted for validation (grep for both patterns)
</verification>

<success_criteria>
- Training script is complete and runnable (not just a stub)
- Script uses weighted loss for training, unweighted for validation
- Early stopping on unweighted val_loss with configurable patience
- Best and final checkpoints saved with full state (model, optimizer, scheduler, config, class_names, normalization)
- CSV log with epoch, train_loss, val_loss, val_acc, per-class recall, lr
- Loss curve and recall curve PNGs generated
- Malignant collapse warning logged if recall[2] == 0 for 2+ epochs
- `make train-all` trains both split strategies
</success_criteria>

<output>
After completion, create `.planning/phases/04-model-training/04-02-SUMMARY.md`
</output>
