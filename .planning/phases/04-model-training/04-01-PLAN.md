---
phase: 04-model-training
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/classifier.py
  - src/models/factory.py
  - src/models/__init__.py
autonomous: true

must_haves:
  truths:
    - "Importing BTXRDClassifier and calling forward produces valid 3-class logits"
    - "create_model(config) returns a configured EfficientNet-B0 with correct num_classes and dropout"
    - "save_checkpoint writes a .pt file containing model weights, optimizer state, config, class names, and normalization stats"
    - "load_checkpoint reads a .pt file and returns a dict that can restore model state"
    - "compute_class_weights returns inverse-frequency weights matching verified values"
    - "get_device returns the correct torch.device for auto/cpu/cuda/mps strings"
  artifacts:
    - path: "src/models/classifier.py"
      provides: "BTXRDClassifier nn.Module wrapping timm EfficientNet-B0"
      contains: "class BTXRDClassifier"
    - path: "src/models/factory.py"
      provides: "Model factory: create_model, save_checkpoint, load_checkpoint, compute_class_weights, get_device, EarlyStopping"
      exports: ["create_model", "save_checkpoint", "load_checkpoint", "compute_class_weights", "get_device", "EarlyStopping"]
    - path: "src/models/__init__.py"
      provides: "Public exports from models package"
      contains: "BTXRDClassifier"
  key_links:
    - from: "src/models/classifier.py"
      to: "timm.create_model"
      via: "timm.create_model('efficientnet_b0', pretrained=True, num_classes=3, drop_rate=0.2)"
      pattern: "timm\\.create_model"
    - from: "src/models/factory.py"
      to: "src/models/classifier.py"
      via: "imports BTXRDClassifier to create model instances"
      pattern: "BTXRDClassifier"
    - from: "src/models/factory.py"
      to: "torch.save / torch.load"
      via: "checkpoint serialization with map_location and weights_only=False"
      pattern: "torch\\.save|torch\\.load"
---

<objective>
Implement the model architecture module (BTXRDClassifier wrapping timm EfficientNet-B0) and the model factory (create_model, checkpoint save/load, class weight computation, device detection, early stopping).

Purpose: These are the foundational building blocks that the training script (Plan 04-02) imports. Separating model definition from training logic keeps the codebase modular and enables Phase 5 (eval) and Phase 6 (Grad-CAM/inference) to import the same model and checkpoint utilities.

Output: Three production-ready Python modules: `classifier.py`, `factory.py`, updated `__init__.py`.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-model-training/04-RESEARCH.md
@configs/default.yaml
@src/models/classifier.py
@src/models/factory.py
@src/models/__init__.py
@src/data/transforms.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement BTXRDClassifier in classifier.py</name>
  <files>src/models/classifier.py</files>
  <action>
Replace the placeholder stub in `src/models/classifier.py` with the full BTXRDClassifier implementation.

The class must:
- Inherit from `torch.nn.Module`
- Accept constructor args: `backbone: str = "efficientnet_b0"`, `num_classes: int = 3`, `pretrained: bool = True`, `drop_rate: float = 0.2`
- Create the timm model via `timm.create_model(backbone, pretrained=pretrained, num_classes=num_classes, drop_rate=drop_rate)`
- Store `self.num_classes = num_classes` and `self.num_features = self.model.num_features` (1280 for efficientnet_b0)
- `forward(self, x: torch.Tensor) -> torch.Tensor` delegates to `self.model(x)` and returns raw logits (NOT softmax -- CrossEntropyLoss expects logits)
- `gradcam_target_layer` property returns `self.model.bn2` (BatchNormAct2d 1280, confirmed in research as the Grad-CAM target for Phase 6)

Include module docstring referencing timm 1.0.15, verified parameter count 4,011,391, Linear(1280, 3) classifier head.

Do NOT apply softmax in forward(). Do NOT freeze any layers (full fine-tuning).
  </action>
  <verify>
Run: `python -c "import sys; sys.path.insert(0, '.'); from src.models.classifier import BTXRDClassifier; import torch; m = BTXRDClassifier(pretrained=False); x = torch.randn(2, 3, 224, 224); out = m(x); assert out.shape == (2, 3), f'Expected (2,3) got {out.shape}'; print(f'OK: output shape {out.shape}, num_features={m.num_features}, num_classes={m.num_classes}'); print(f'Grad-CAM layer: {type(m.gradcam_target_layer).__name__}')"` -- must print OK with shape (2,3), num_features=1280, num_classes=3, and Grad-CAM layer type.
  </verify>
  <done>BTXRDClassifier produces (batch, 3) logits from (batch, 3, 224, 224) input. num_features=1280. gradcam_target_layer is accessible.</done>
</task>

<task type="auto">
  <name>Task 2: Implement model factory, checkpoint utilities, class weights, device detection, and early stopping in factory.py</name>
  <files>src/models/factory.py</files>
  <action>
Replace the placeholder stub in `src/models/factory.py` with the following functions and class:

**`get_device(device_str: str = "auto") -> torch.device`**
- Priority: CUDA > MPS > CPU when `device_str == "auto"`
- Otherwise interpret directly via `torch.device(device_str)`

**`create_model(config: dict) -> BTXRDClassifier`**
- Reads `config["model"]` section: backbone, num_classes, pretrained, dropout
- Returns `BTXRDClassifier(backbone, num_classes, pretrained, drop_rate=dropout)`
- Import BTXRDClassifier from `.classifier`

**`compute_class_weights(labels: list[int], num_classes: int = 3) -> torch.Tensor`**
- Formula: `weight_c = n_samples / (n_classes * count_c)` (identical to sklearn balanced)
- Uses `np.bincount(labels, minlength=num_classes)`
- Returns `torch.tensor(weights, dtype=torch.float32)` (CPU tensor -- caller moves to device)

**`save_checkpoint(model, optimizer, scheduler, epoch, val_loss, config, class_weights, filepath)`**
- Saves dict with keys: `epoch`, `model_state_dict`, `optimizer_state_dict`, `scheduler_state_dict` (None if scheduler is None), `val_loss`, `config`, `class_names` (from config["model"]["class_names"]), `normalization` (dict with mean/std from `src.data.transforms.IMAGENET_MEAN/STD`), `class_weights` (as list via `.tolist()`)
- Creates parent directory via `Path(filepath).parent.mkdir(parents=True, exist_ok=True)`
- Uses `torch.save(checkpoint, filepath)`

**`load_checkpoint(filepath: str, device: str = "cpu") -> dict`**
- Uses `torch.load(filepath, map_location=device, weights_only=False)`
- Returns the checkpoint dict

**`class EarlyStopping`**
- `__init__(self, patience: int = 7, min_delta: float = 0.0)`
- Tracks `best_loss`, `counter`, `should_stop`
- `__call__(self, val_loss: float) -> bool`: returns True when patience exceeded
- If `val_loss < best_loss - min_delta`, reset counter and update best_loss; else increment counter

Imports needed: `torch`, `torch.nn`, `numpy`, `pathlib.Path`, and from local modules: `BTXRDClassifier` from `.classifier`, `IMAGENET_MEAN`, `IMAGENET_STD` from `src.data.transforms`.
  </action>
  <verify>
Run: `python -c "
import sys; sys.path.insert(0, '.')
from src.models.factory import create_model, compute_class_weights, get_device, save_checkpoint, load_checkpoint, EarlyStopping
import torch

# Test get_device
d = get_device('cpu')
assert str(d) == 'cpu'
d_auto = get_device('auto')
print(f'Auto device: {d_auto}')

# Test create_model
cfg = {'model': {'backbone': 'efficientnet_b0', 'num_classes': 3, 'pretrained': False, 'dropout': 0.2}}
m = create_model(cfg)
assert m.num_classes == 3

# Test compute_class_weights
w = compute_class_weights([0]*1315 + [1]*1066 + [2]*240)
print(f'Weights: {w.tolist()}')
assert abs(w[2].item() - 3.640) < 0.01, f'Malignant weight {w[2]} != ~3.640'

# Test EarlyStopping
es = EarlyStopping(patience=3)
assert not es(1.0)
assert not es(0.9)
assert not es(1.0)
assert not es(1.0)
assert es(1.0)  # 3 non-improvements -> stop
print('All factory tests passed')
"` -- must print weights close to [0.664, 0.820, 3.640] and 'All factory tests passed'.
  </verify>
  <done>All factory functions work: create_model returns configured BTXRDClassifier, compute_class_weights produces correct inverse-frequency weights, get_device resolves auto/cpu/cuda/mps, save/load_checkpoint round-trip works, EarlyStopping fires after patience exhausted.</done>
</task>

<task type="auto">
  <name>Task 3: Update models __init__.py with public exports</name>
  <files>src/models/__init__.py</files>
  <action>
Update `src/models/__init__.py` to export the public API from classifier.py and factory.py:

```python
"""Model architectures and factory for bone tumor classification."""

from src.models.classifier import BTXRDClassifier
from src.models.factory import (
    EarlyStopping,
    compute_class_weights,
    create_model,
    get_device,
    load_checkpoint,
    save_checkpoint,
)

__all__ = [
    "BTXRDClassifier",
    "create_model",
    "save_checkpoint",
    "load_checkpoint",
    "compute_class_weights",
    "get_device",
    "EarlyStopping",
]
```
  </action>
  <verify>
Run: `python -c "import sys; sys.path.insert(0, '.'); from src.models import BTXRDClassifier, create_model, compute_class_weights, get_device, EarlyStopping, save_checkpoint, load_checkpoint; print('All imports OK')"` -- must print 'All imports OK'.
  </verify>
  <done>src/models/__init__.py exports all public symbols. Downstream code (train.py, eval.py, infer.py) can import from src.models directly.</done>
</task>

</tasks>

<verification>
After all tasks complete, run the combined verification:

```bash
python -c "
import sys; sys.path.insert(0, '.')
import torch
from src.models import BTXRDClassifier, create_model, compute_class_weights, get_device, EarlyStopping, save_checkpoint, load_checkpoint

# 1. Model creation from config
cfg = {'model': {'backbone': 'efficientnet_b0', 'num_classes': 3, 'pretrained': False, 'dropout': 0.2}}
model = create_model(cfg)
assert isinstance(model, BTXRDClassifier)
assert model.num_classes == 3
assert model.num_features == 1280

# 2. Forward pass
x = torch.randn(2, 3, 224, 224)
out = model(x)
assert out.shape == (2, 3)

# 3. Grad-CAM target layer exists
layer = model.gradcam_target_layer
assert layer is not None

# 4. Class weights
w = compute_class_weights([0]*1315 + [1]*1066 + [2]*240)
assert abs(w[0].item() - 0.664) < 0.01
assert abs(w[1].item() - 0.820) < 0.01
assert abs(w[2].item() - 3.640) < 0.01

# 5. Device detection
d = get_device('cpu')
assert str(d) == 'cpu'

# 6. Checkpoint round-trip
import tempfile, os
with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as f:
    tmp = f.name
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
cfg['model']['class_names'] = ['Normal', 'Benign', 'Malignant']
save_checkpoint(model, optimizer, None, 1, 0.5, cfg, w, tmp)
ckpt = load_checkpoint(tmp)
assert 'model_state_dict' in ckpt
assert ckpt['class_names'] == ['Normal', 'Benign', 'Malignant']
assert 'normalization' in ckpt
os.unlink(tmp)

print('ALL VERIFICATION PASSED')
"
```

Must print `ALL VERIFICATION PASSED`.
</verification>

<success_criteria>
- BTXRDClassifier wraps timm EfficientNet-B0 with 3-class logit output
- Factory functions (create_model, save/load_checkpoint, compute_class_weights, get_device, EarlyStopping) are tested and working
- All symbols importable from `src.models`
- No new dependencies (timm, torch, numpy already in requirements.txt)
</success_criteria>

<output>
After completion, create `.planning/phases/04-model-training/04-01-SUMMARY.md`
</output>
